{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP 2026\n",
    "# Lab 2: Word Vectors and Information Retrieval\n",
    "## *alt*-title: üöÄ Project CleanSearch AI, a DOGE initiative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## üèõÔ∏èüêï PRESS RELEASE ‚Äî For Immediate (and Maximum Efficiency) Distribution  \n",
    "\n",
    "### The Department of Outdated Government Encyclopedias (DOGE) Launches Revolutionary NLP Project to Rescue Public Knowledge  \n",
    "\n",
    "**Washington, D.C.** ‚Äî In a bold step toward modernizing the nation‚Äôs most chaotic digital archives, the   **Department of Outdated Government Encyclopedias (DOGE)** today announced the launch of its new initiative:  üöÄ **Project CleanSearch AI**.\n",
    "\n",
    "For decades, citizens have struggled to find simple answers hidden inside massive, noisy, and poorly structured government knowledge repositories.\n",
    "\n",
    "Questions such as:\n",
    "\n",
    "- ‚ÄúWho won the Nobel Prize in 1930?‚Äù  \n",
    "- ‚ÄúWhen did Angola become independent?‚Äù  \n",
    "\n",
    "have resulted in thousands of irrelevant web pages, confusing biographies and excessive scrolling üìâ\n",
    "\n",
    "> *‚ÄúFrankly, our archives are a mess,‚Äù* said a DOGE spokesperson.  \n",
    "> *‚ÄúThey‚Äôre long, noisy and about as searchable as a pile of printed Wikipedia pages thrown into a hurricane.‚Äù*\n",
    "\n",
    "### üß† The Solution  \n",
    "\n",
    "DOGE has assembled an elite team of AI specialists, hired from UM DACS 2nd year bachelor program with the following goals:\n",
    "\n",
    "‚úÖ Clean decades of messy digital text  \n",
    "‚úÖ Extract meaningful knowledge  \n",
    "‚úÖ Replace outdated keyword search with modern **retrieval systems**  \n",
    "‚úÖ Deliver instant, accurate answers to citizens  \n",
    "\n",
    "Using real-world noisy data similar to the government‚Äôs archives, the team will experiment with multiple retrieval models to determine the most efficient approach, methods which have been taught in the fabulous classes of some person quoted as J.S. \n",
    "\n",
    "Whispers across the digital corridors suggest that DOGE may soon supercede the legendary Project 2-2, though DACS management insist these rumours are ‚Äúunder control.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deliverable:\n",
    "\n",
    "- You are asked to deliver **two files only**:\n",
    "  - your executed notebook file (`.ipynb`), and\n",
    "  - your poster (`.pdf`).  \n",
    "  No other files will be taken into consideration.\n",
    "  \n",
    "‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è Each part of the poster will contribute to your grade proportionally to what we present below. If we can't find the relevant part in your notebook (e.g. the figure or the code to support your findings) we will reduce (or even zero-out) your grade for that part.\n",
    "\n",
    "### Instructions for the poster: \n",
    "\n",
    "The final deliverable for this lab is a **scientific poster** presenting your work on building and evaluating a sentence retrieval system using the TriviaQA dataset.\n",
    "- üìè **Size:** A0 or A1  \n",
    "- üß≠ **Orientation:** Portrait or landscape (your choice)  \n",
    "- üìë **Layout:** Clear section structure (e.g., columns or blocks)  \n",
    "\n",
    "#### Your poster should include the following sections:\n",
    "---\n",
    "#### 1Ô∏è‚É£ Problem & Motivation üéØ\n",
    "- Describe the retrieval task (query ‚Üí correct answer document) and the challenges\n",
    "- Briefly introduce the dataset and its challenges  \n",
    "#### 2Ô∏è‚É£ Data Preparation üßπ\n",
    "Explain:\n",
    "- Train / validation / test splitting  \n",
    "- Your cleaning pipeline (at least 6 preprocessing steps)  \n",
    "Include at least one **before vs after cleaning** example.\n",
    "#### 3Ô∏è‚É£ Retrieval Models ü§ñ\n",
    "Present and explain the modes you used:\n",
    "- Bag-of-Words + cosine similarity  \n",
    "- TF-IDF + cosine similarity  \n",
    "- Sentence embeddings (averaged word embeddings)  \n",
    "- [any other model?] \n",
    "Discuss strengths and limitations of each.\n",
    "#### 4Ô∏è‚É£ Qualitative Analysis üîç\n",
    "Provide:\n",
    "- At least **3 successful retrieval examples**  \n",
    "- At least **3 failure cases**  \n",
    "Explain why each worked or failed.\n",
    "#### 5Ô∏è‚É£ Quantitative Evaluation üìä (Main focus)\n",
    "Report **Recall@K** (and possibly other metrics) on the **test set** for all methods:\n",
    "- BOW  \n",
    "- TF-IDF  \n",
    "- Pre-trained embeddings  \n",
    "- [Additional models]  \n",
    "Include relevant table(s) and/or plot(s) and briefly discuss trends.\n",
    "#### 6Ô∏è‚É£ Discussion & Recommendations üí°\n",
    "Conclude with:\n",
    "- Which method you would recommend and why  \n",
    "- Key tradeoffs  \n",
    "- Possible improvements  \n",
    "### üé® Optional Creative Element (Bonus)\n",
    "\n",
    "You may (optionally) present your poster within the fictional storyline of üèõÔ∏è **DOGE ‚Äî Department of Outdated Government Encyclopedias**, where your retrieval system modernizes chaotic national archives and replaces legacy keyword search. Creativity is welcome, but scientific clarity is the priority. We will vote for the \"most creative poster\".\n",
    "\n",
    "---\n",
    "\n",
    "### üìè Evaluation Focus\n",
    "\n",
    "Posters will be assessed on:\n",
    "- Correctness of the pipeline incl. the code (25%)\n",
    "- Clarity of explanations and interpretations of results (25%)\n",
    "- Quality of analysis (20%)\n",
    "- Proper use of evaluation metrics (e.g. Recall@K) (10%)\n",
    "- Visual organization (10%)\n",
    "- Discussion and recommendations (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "As in the last lab, we will be using huggingface datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). We will work with TriviaQA dataset ([https://huggingface.co/datasets/sentence-transformers/trivia-qa](https://huggingface.co/datasets/sentence-transformers/trivia-qa)), which contains pairs of queries and articles that contain the answer.\n",
    "\n",
    "In this section we will prepare the dataset, aka clean the sentences and tokenize. We will additionally extract the answers, as some articles correspond to multiple queries. We will create a separate dataset from the unique answers. We will do that for each split separately, so that we can test our retrieval fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start with importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from datasets import DatasetDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading\n",
    "Now, we can begin loading the dataset and inspecting the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 73346\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('sentence-transformers/trivia-qa')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?', 'answer': 'The Nobel Prize in Literature 1930 The Nobel Prize in Literature 1930 Sinclair Lewis The Nobel Prize in Literature 1930 Sinclair Lewis Prize share: 1/1 The Nobel Prize in Literature 1930 was awarded to Sinclair Lewis \"for his vigorous and graphic art of description and his ability to create, with wit and humour, new types of characters\". Photos: Copyright ¬© The Nobel Foundation Share this: To cite this page MLA style: \"The Nobel Prize in Literature 1930\". Nobelprize.org. Nobel Media AB 2014. Web. 18 Jan 2017. <http://www.nobelprize.org/nobel_prizes/literature/laureates/1930/>'}\n",
      "{'query': 'Where in England was Dame Judi Dench born?', 'answer': 'Judi Dench - IMDb IMDb Actress | Music Department | Soundtrack Judi Dench was born in York, England, to Eleanora Olive (Jones), who was from Dublin, Ireland, and Reginald Arthur Dench, a doctor from Dorset, England. She attended Mount School in York, and studied at the Central School of Speech and Drama. She has performed with Royal Shakespeare Company, the National Theatre, and at Old Vic Theatre. She is a ... See full bio ¬ª Born: a list of 35 people created 02\\xa0Jul\\xa02011 a list of 35 people created 19\\xa0Apr\\xa02012 a list of 35 people created 28\\xa0May\\xa02014 a list of 25 people created 05\\xa0Aug\\xa02014 a list of 26 people created 18\\xa0May\\xa02015 Do you have a demo reel? Add it to your IMDbPage How much of Judi Dench\\'s work have you seen? User Polls Won     1     Oscar. Another    59 wins & 163 nominations. See more awards \\xa0¬ª Known For \\xa02016 The Hollow Crown (TV Series) Cecily, Duchess of York \\xa02015 The Vote (TV Movie) Christine Metcalfe - Total War (1996) ... Narrator (voice) - Stalemate (1996) ... Narrator (voice) \\xa01992 The Torch (TV Mini-Series) Aba \\xa01990 Screen One (TV Series) Anne \\xa01989 Behaving Badly (TV Mini-Series) Bridget \\xa01981 BBC2 Playhouse (TV Series) Sister Scarli \\xa01976 Arena (TV Series documentary) Sweetie Simpkins \\xa01973 Ooh La La! (TV Series) Am√©lie \\xa01966 Court Martial (TV Series) Marthe \\xa01963 Z Cars (TV Series) Elena Collins \\xa01963 Love Story (TV Series) Pat McKendrick \\xa01960 The Terrible Choice (TV Series) Good Angel Music department (1 credit) \\xa0 A Fine Romance (TV Series) (theme sung by - 14 episodes, 1981 - 1983) (theme song sung by - 12 episodes, 1983 - 1984) - A Romantic Meal (1984) ... (theme song sung by) - Problems (1984) ... (theme song sung by) \\xa02013 Fifty Years on Stage (TV Movie) (performer: \"Send in the Clowns\") \\xa02009 Nine (performer: \"Folies Berg√®re\") - What\\'s Wrong with Mrs Bale? (1997) ... (performer: \"Raindrops Keep Fallin\\' On My Head\" - uncredited) - Misunderstandings (1993) ... (performer: \"Walkin\\' My Baby Back Home\" - uncredited) \\xa01982-1984 A Fine Romance (TV Series) (performer - 2 episodes) - The Telephone Call (1984) ... (performer: \"Boogie Woogie Bugle Boy\" - uncredited) - Furniture (1982) ... (performer: \"Rule, Britannia!\" - uncredited) Hide\\xa0 \\xa02009 Waiting in Rhyme (Video short) (special thanks) \\xa02007 Expresso (Short) (special thanks) \\xa01999 Shakespeare in Love and on Film (TV Movie documentary) (thanks - as Dame Judi Dench) Hide\\xa0 \\xa02016 Rio Olympics (TV Mini-Series) Herself \\xa02015 In Conversation (TV Series documentary) Herself \\xa02015 Entertainment Tonight (TV Series) Herself \\xa02015 CBS This Morning (TV Series) Herself - Guest \\xa02015 The Insider (TV Series) Herself \\xa01999-2014 Cinema 3 (TV Series) Herself \\xa02013 Good Day L.A. (TV Series) Herself - Guest \\xa02013 Arena (TV Series documentary) Herself \\xa02013 At the Movies (TV Series) Herself \\xa02013 Shooting Bond (Video documentary) Herself \\xa02013 Bond\\'s Greatest Moments (TV Movie documentary) Herself \\xa02012 Made in Hollywood (TV Series) Herself \\xa01999-2012 Charlie Rose (TV Series) Herself - Guest \\xa02008-2012 This Morning (TV Series) Herself - Guest \\xa02012 The Secrets of Skyfall (TV Short documentary) Herself \\xa02012 Anderson Live (TV Series) Herself \\xa02012 J. Edgar: A Complicated Man (Video documentary short) Herself \\xa02011 The Many Faces of... (TV Series documentary) Herself / Various Characters \\xa02011 Na plov√°rne (TV Series) Herself \\xa02010 BBC Proms (TV Series) Herself \\xa02010 The South Bank Show Revisited (TV Series documentary) Herself - Episode #6.68 (2009) ... Herself - Guest (as Dame Judi Dench) \\xa02007-2009 Breakfast (TV Series) \\xa02009 Larry King Live (TV Series) Herself - Guest \\xa02009 The One Show (TV Series) Herself \\xa02009 Cranford in Detail (Video documentary short) Herself / Miss Matty Jenkins (as Dame Judi Dench) \\xa02005-2008 The South Bank Show (TV Series documentary) Herself \\xa02008 Tavis Smiley (TV Series) Herself - Guest \\xa02007 ITV News (TV Series) Herself - BAFTA Nominee \\xa02007 The Making of Cranford (Video documentary short) Herself / Miss Matty Jenkyns (as Dame Judi Dench) \\xa02006 Becoming Bond (TV Movie documentary) Herself \\xa02006 Coraz√≥n de... (TV Series) Hers'}\n",
      "{'query': 'In which decade did Billboard magazine first publish and American hit chart?', 'answer': 'The US Billboard song chart The US Billboard song chart Search this site with Google Song chart US Billboard The Billboard magazine has published various music charts starting (with sheet music) in 1894, the first \"Music Hit Parade\" was published in 1936 , the first \"Music Popularity Chart\" was calculated in 1940 . These charts became less irregular until the weekly \"Hot 100\" was started in 1958 . The current chart combines sales, airplay and downloads. A music collector that calls himself Bullfrog has been consolidating the complete chart from 1894 to the present day.  he has published this information in a comprehenive spreadsheet (which can be obtained at bullfrogspond.com/ ). The Bullfrog data assigns each song a unique identifier, something like \"1968_076\" (which just happens to be the Bee Gees song \"I\\'ve Gotta Get A Message To You\"). This \"Whitburn Number\" is provided to match with the books of Joel Whitburn and consists of the year and a ranking within the year. A song that first entered the charts in December and has a long run is listed the following year. This numbering scheme means that songs which are still in the charts cannot be assigned a final id, because their ranking might change. So the definitive listing for a year cannot be final until about April. In our listing we only use songs with finalised IDs, this means that every year we have to wait until last year\\'s entries are finalised before using them. (Source bullfrogspond.com/ , the original version used here was 20090808 with extra data from: the 2009 data from 20091219 the 2010 data from 20110305 the 2011 data from 20120929 the 2012 data from 20130330 the 2013 data from 20150328 The 20150328 data was the last one produced before the Billboard company forced the data to be withdrawn. As far as we know there are no more recent data sets available. This pattern of obtaining the data for a particular year in the middle of the following one comes from the way that the Bullfrog project generates the identifier for a song (what they call the \"Prefix\" in the spreadsheet). Recent entries are identified with keys like \"2015-008\" while older ones have keys like \"2013_177\". In the second case the underscore is significant, it indicates that this was the 177th biggest song released in 2013. Now, of course, during the year no one knows where a particular song will rank, so the underscore names can\\'t be assigned until every song from a particular year has dropped out of the charts, so recent records are temporarily assigned a name with a dash. In about May of the following year the rankings are calculated and the final identifiers are assigned. That is why we at the Turret can only grab this data retrospectively. Attributes The original spreadsheet has a number of attributes, we have limited our attention to just a few of them: 134 9 The songs with the most entries on the chart were White Christmas (with 33 versions and a total of 110 weeks) and Stardust (with 19 and a total of 106 weeks). position The peak position that songs reached in the charts should show an smooth curve from number one down to the lowest position. This chart has more songs in the lower peak positions than one would expect. Before 1991 the profile of peak positions was exactly as you would expect, that year Billboard introduced the concept of \"Recurrent\" tracks, that is they removed any track from the chart which had spent more than twenty weeks in the chart and had fallen to the lower positions. weeks The effect of the \"Recurrent\" process, by which tracks are removed if they have spent at least twenty weeks in the chart and have fallen to the lower reaches, can clearly be seen in the strange spike in this attribute. This \"adjustment\" was intended to promote newer songs and ensure the chart does not become \"stale\". In fact since it was introduced in 1991 the length of long chart runs has increased, this might reflect the more conscious efforts of record companies to \"game\" the charts by controlling release times and promotions, or it coul'}\n",
      "{'query': 'From which country did Angola achieve independence in 1975?', 'answer': \"Angola from past to present | Conciliation Resources Angola from past to present Angola from past to present From military peace to social justice? The Angolan peace process Publication date:\\xa0 David Birmingham When Angola achieved independence in 1975, a war was raging between competing national liberation movements and their foreign backers. Guus Meijer and David Birmingham revisit Angola‚Äôs colonial period and the independence struggle that followed and ask how the resulting social and economic divisions shaped and were manipulated by the warring parties. The article describes the introduction of authoritarian one-party rule under the MPLA and the impact of natural resource development and international and regional powers on the conflict. Tracing the conflict up to the signing of the Luena Memorandum, the authors conclude that Angola‚Äôs peace remains incomplete and that the country faces many challenges in achieving social and democratic reconstruction. Read full article Angola from past to present On 11 November 1975, the Popular Movement for the Liberation of Angola (MPLA) declared Angola's independence and installed Agostinho Neto as its first President in the former Portuguese colony's capital at Luanda. This outcome had long seemed uncertain and indeed even unlikely; the MPLA had not only had to deal with its own serious internal troubles and disaffections, but had also had to take on the Portuguese colonial army and the two rival armed movements, each backed by powerful allies. Holden Roberto's National Front for the Liberation of Angola (FNLA) had initially been the most powerful of the three competing national liberation movements and in the autumn of 1975 it came close to capturing Luanda from the north, backed by a heavily armed force supplied by President Mobuto Sese Seko of Zaire (now the Democratic Republic of Congo). In the south, two armoured columns of a South African invasion force, acting in military coordination with the Union for the Total Independence of Angola (UNITA), led by Jonas Savimbi, almost reached Luanda before they were stopped by Cuban troops which had been rushed to the assistance of the MPLA. The independent Angolan state was thus born out of turmoil and violence and amid serious national, regional and global rivalries. This heritage with its deep historical roots was to influence the unfolding of events for a long time. Angola, like most African countries, grew out of a conglomerate of peoples and groups each with its own distinct history and traditions. Gradually small local nations and states came into contact with each other and historical developments drove them to share a common destiny under increasing Portuguese influence. Long before the arrival of the Portuguese, Bantu-speaking communities had established a farming economy over most of the territory. They had absorbed many of the scattered Khoisan-speaking populations and developed a successful pastoral dimension to their agriculture as well as building up trading economies. One of the most successfully diverse market centres became the town of M'banza Kongo around which the Kongo kingdom evolved. Further east the concept of state formation related to the political ideology of the Lunda peoples while in the south later kingdoms took shape in the highlands of the Ovimbundu people. Angola under Portuguese rule Although the first Portuguese traders, explorers and soldiers set foot on this part of the African coast from 1483, modern colonisation of the whole territory was only formalised four centuries later after the Berlin Conference of 1884-85. Wide stretches of Angola experienced colonial rule for less than a century, and even after 1900 armed revolts broke out and resistance movements sprang up as among the Ovimbundu and the Bakongo from 1913, until the last northern resistance was put down in 1917. During its century of overrule the colonial regime left crucial marks on Angolan society. Its discriminatory legislation, particularly the Statute of the Portuguese Natives of the Provinces of Angola, Mozambique, and Guinea, separ\"}\n",
      "{'query': 'Which city does David Soul come from?', 'answer': 'David Soul - IMDb IMDb Actor | Soundtrack | Director David Soul achieved pop icon status as handsome, blond-haired, blue-eyed Detective Kenneth Hutchinson on the cult \"buddy cop\" TV series Starsky and Hutch (1975), Soul also had a very successful singing career recording several albums, with worldwide number one hit singles including \"Silver Lady\" & \"Don\\'t Give Up on Us Baby\". Born in Chicago, ... See full bio ¬ª Born: Share this page: Related News a list of 43 people created 14\\xa0Jan\\xa02011 a list of 37 people created 13\\xa0Mar\\xa02011 a list of 48 people created 26\\xa0Mar\\xa02012 a list of 973 people created 26\\xa0Feb\\xa02013 a list of 127 people created 05\\xa0Jul\\xa02014 Do you have a demo reel? Add it to your IMDbPage How much of David Soul\\'s work have you seen? User Polls 1 win & 3 nominations. See more awards \\xa0¬ª Known For Starsky and Hutch Det. Ken \\'Hutch\\' Hutchinson (1975-1979) \\xa02004 The Dark Lantern (TV Movie) Storyteller \\xa02004 Dalziel and Pascoe (TV Series) Detective Gus D\\'Amato \\xa01995 Vents contraires (TV Movie) Quill \\xa01994 High Tide (TV Series) Brian Landis \\xa01991-1993 Murder, She Wrote (TV Series) Jordan Barnett / Wes McSorley \\xa01990 The Young Riders (TV Series) Jeremy Styles \\xa01989 Prime Target (TV Movie) Peter Armetage \\xa01989 Deadly Nightmares (TV Series) Cooper Halliday \\xa01989 Alfred Hitchcock Presents (TV Series) Michael Dennison \\xa01987 Crime Story (TV Series) Dr. Newhouse \\xa01987 Harry\\'s Hong Kong (TV Movie) Harry Petros \\xa01986 The Fifth Missile (TV Movie) Capt. Kevin Harris \\xa01984 Partners in Crime (TV Series) Harry \\xa01983 Through Naked Eyes (TV Movie) William Parrish \\xa01982 World War III (TV Movie) Col. Jake Caffey \\xa01980 Homeward Bound (TV Movie) Jake Seaton \\xa01980 Swan Song (TV Movie) Jesse Swan \\xa01974 Medical Center (TV Series) Walter \\xa01974 McMillan & Wife (TV Series) Jerry \\xa01974 The Rookies (TV Series) Johnny Dane \\xa01973 Circle of Fear (TV Series) James Barlow \\xa01972 The F.B.I. (TV Series) Clifford Wade \\xa01972 Movin\\' On (TV Movie) Jeff \\xa01971 Dan August (TV Series) Lawrence Merrill III \\xa01967 Star Trek (TV Series) Makora \\xa02016 The Conjuring 2 (performer: \"Don\\'t Give Up On Us\") \\xa02013/I Filth (performer: \"Silver Lady\") \\xa02011 Johnny English Reborn (courtesy: \"Don\\'t Give Up On Us\") / (performer: \"Don\\'t Give Up On Us\") \\xa02010 Rabbit Hole (performer: \"Don\\'t Give Up On Us\") \\xa02007 The Hitcher (performer: \"Don\\'t Give Up on Us\") \\xa01977-1978 Top of the Pops (TV Series) (performer - 17 episodes) - Episode dated 22 June 1978 (1978) ... (performer: \"It Sure Brings Out the Love in Your Eyes\") - Episode dated 8 June 1978 (1978) ... (performer: \"It Sure Brings Out the Love in Your Eyes\")'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Splitting\n",
    "\n",
    "You might have noticed that the dataset is not split into subsets (it contains only the `train` subset). To maintain the good practice of working with ML, we should have three datasets: `train`, `validation`, and `test`. The code below splits our dataset into those three subsets. We set the size of both the `validation` and `test` sets as 10,000 and keep the rest in the `train` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 53346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=10_000)\n",
    "valid_dataset = dataset['test']\n",
    "dataset = dataset['train'].train_test_split(test_size=10_000)\n",
    "dataset['validation'] = valid_dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning\n",
    "\n",
    "Let's write the function to clean the text. It can be similar to the one from the previous lab (Lab1) but make sure that it makes sense for this dataset and task.\n",
    "\n",
    "More specifically, think about lower-casing, punctuation, stop-words and lemmatization/stemming and the impact it might have on the dataset. Also reflect on the fact that with word embeddings we want to uncover semantic relationships between words, whereas with bag-of-words we were trying to capture different morphological variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise 1: Clean function\n",
    "Fill in the following function to clean the dataset. Implement at least 6 different steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the clean function:\n",
      "Original: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
      "Cleaned: which american born sinclair won nobel prize literature 1930\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Cleans the text\n",
    "    Args:\n",
    "        text: a string that will be cleaned\n",
    "\n",
    "    Returns: the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        return text\n",
    "\n",
    "    text = str(text)\n",
    "    text = text.replace('\\\\xa0', ' ')\n",
    "\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove url and html parts\n",
    "    text = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', ' ', text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    # to normalize apostroph and separators\n",
    "    text = re.sub(r'[-_/]', ' ', text)\n",
    "    text = text.replace(\"‚Äô\", \"'\")\n",
    "    text = re.sub(r\"\\\\b(\\\\w+)'s\\\\b\", r'\\\\1', text)\n",
    "\n",
    "    # remove punctuation symbols\n",
    "    text = re.sub(r\"[^a-z0-9\\\\s']\", ' ', text)\n",
    "    text = text.replace(\"'\", '')\n",
    "\n",
    "    # remove most common stopwords\n",
    "    stop_words = {\n",
    "        'a', 'an', 'the', 'and', 'or', 'but', 'if', 'to', 'of', 'in', 'on',\n",
    "        'for', 'by', 'with', 'as', 'at', 'from', 'is', 'are', 'was', 'were'\n",
    "    }\n",
    "    tokens = [tok for tok in text.split() if tok not in stop_words]\n",
    "\n",
    "    # to normalize whitespaces\n",
    "    text = ' '.join(tokens)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "sentence = 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?'\n",
    "print('Testing the clean function:')\n",
    "print('Original:', sentence)\n",
    "print('Cleaned:', clean(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function will apply the function you just wrote to the whole dataset. More specifically, it takes the `query` and `answer` fields, applies the `clean` function and saves the processed sentences back to the `query` and `answer` fields. This will override the original fields. If you want to have access to them, you can make a copy in separate fields before cleaning. As in the last lab, we will use the `map()` method of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab93f08c434543e4a196409f623a08ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning queries and answers:   0%|          | 0/53346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f96295c65904fa496e10c661a019e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning queries and answers:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a6312d37a743308a40c4b04d3f5344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning queries and answers:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 53346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def clean_example(example):\n",
    "    \"\"\"\n",
    "    Applies the clean() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example with cleaned 'query' and 'answer' columns\n",
    "\n",
    "    \"\"\"\n",
    "    # example['original_query'] = example['query']\n",
    "    # example['original_answer'] = example['answer']\n",
    "\n",
    "    example['query'] = clean(example['query'])\n",
    "    example['answer'] = clean(example['answer'])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(clean_example, desc=\"Cleaning queries and answers\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine some examples from the dataset and make sure that we got the results we wanted. At this step, it might be necessary to revisit some pre-processing steps if you are not happy with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0\n",
      "query deriving turkish word maidservant what french name given female slave harem\n",
      "answer online etymology dictionary occupy v mid 14c take possession also take up space time employ someone irregularly borrowed old french occuper occupy person place hold seize 13c directly latin occupare take over seize take into possession possess occupy ob over see ob intensive form capere grasp seize pie root kap grasp see capable final syllable english word difficult explain it old record perhaps modification made anglo french during 16c 17c common euphemism have sexual intercourse sense attested early 15c which caused it fall polite usage captaine gods light these villaines wil make word odious word occupy which excellent good worde before it il sorted doll tearsheet 2 henry iv occur v 1520s meet meet argument middle french occurrer happen unexpectedly directly latin occurrere run meet run against befall present itself ob against toward see ob currere run see current adj sense development meet present itself appear happen present itself course events meaning come into ones mind 1620s related occurred occurring ocean n late 13c old french occean ocean 12c modern french oc latin oceanus greek okeanos great river sea surrounding disk earth opposed mediterranean unknown origin personified oceanus son uranus gaia husband tethys early times when only known land masses eurasia africa ocean endless river that flowed around them until c 1650 commonly ocean sea translating latin mare oceanum application individual bodies water began 14c there usually reckoned be five them this arbitrary also occasionally applied smaller subdivisions such german ocean north sea oceania southern pacific island australia conceived continent 1849 modern latin french oc anie c 1812 apparently coined danish geographer conrad malte brun 1755 1826 earlier english oceanica 1832 oceania name one superstates orwells nineteen eighty four oceanea name james harringtons 17c ideal state later applied british empire ochlocracy n government rabble 1580s french ochlocratie 1560s greek okhlokratia polybius mob rule lowest grade democracy kratos rule power strength see cracy okhlos mob populace perhaps literally moving mass pie wogh lo root wegh go transport vehicle see weigh sense development compare mob n related ochlocratic ochlocratical greek also had okhlagogos mob leader ochlagogue ochre n type clayey soil much used pigments late 14c old french ocre c 1300 directly late latin ocra latin ochra greek ochra ochros pale yellow unknown origin color name brownish yellow it attested mid 15c related ochreous octa before vowels oct word forming element meaning eight greek okta okt pie okto u eight see eight variant form octo often appears words taken latin greek form said be more common english octave n c 1300 utaves plural via anglo french popular old french form oitieve otaves reformed early 15c medieval latin octava latin octava dies eighth day fem octavus eighth octo see eight originally period eight days after festival also eighth day after festival counting both days inclusive reckoning thus festival sunday octaves would be following sunday verse sense stanza eight lines 1580s musical sense note eight diatonic degrees above below give\n",
      "\n",
      "example 1\n",
      "query what typical subject paintings dgas\n",
      "answer edgar degas 1834 1917 painting drawing essay heilbrunn timeline art history metropolitan museum art metropolitan museum art edgar degas 1834 1917 painting drawing see works art works art 20 essay edgar degas seems never have reconciled himself label impressionist preferring call himself realist independent nevertheless he one groups founders organizer its exhibitions one its most important core members like impressionists he sought capture fleeting moments flow modern life yet he showed little interest painting plein air landscapes favoring scenes theaters caf s illuminated artificial light which he used clarify contours his figures adhering his academic training degas born 1834 scion wealthy banking family educated classics including latin greek ancient history lyc e louis le grand paris his father recognized his sons artistic gifts early encouraged his efforts drawing taking him frequently paris museums degas began copying italian renaissance paintings louvre trained studio louis lamothe who taught traditional academic style its emphasis line its insistence crucial importance draftsmanship degas also strongly influenced paintings frescoes he saw during several long trips italy late 1850s he made many sketches drawings them his notebooks evidence degass classical education can be seen his relatively static frieze like early painting young spartans exercising ca 1860 national gallery london done while he still his twenties yet despite title suggestion classical drapery some figures background there little that places subject this painting ancient greece indeed it has been noted that young girls have snub noses immature bodies montmartre types forerunners dancers degas painted so often throughout his career after 1865 when salon accepted his history painting misfortunes city orl ans mus e dorsay paris degas did not paint academic subjects again focusing his attention scenes modern life he began paint scenes such urban leisure activities horse racing after about 1870 caf concert singers ballet dancers degass choice subject matter reflects his modern approach he favored scenes ballet dancers laundresses milliners milliners 1882 29 100 38 denizens parisian low life his interest ballet dancers intensified 1870s eventually he produced approximately 1 500 works subject these not traditional portraits studies that address movement human body exploring physicality discipline dancers through use contorted postures unexpected vantage points dancer adjusting her slipper 1873 29 100 941 figures pose difficult decipher viewed steep angle both her feet her head bottom picture yet it conveys sense dancers flexibility degas absorbed artistic tradition outside influences reinterpreted them innovative ways following opening trade japan 1854 many french artists including degas increasingly influenced japanese prints whereas his contemporaries often infused their paintings eastern imagery degas abstracted these prints their inventive compositions points view particularly his use cropping asymmetry degas had also observed how sixteenth century italian mannerists similarly framed their subjects sometimes cutting off part figure example woman seated beside vase flowers 1865 29 100 128 figure cut off right edge painting part her left hand just barely visible lower right corner\n",
      "\n",
      "example 2\n",
      "query which part body would russian valenki normally be worn\n",
      "answer traditional dress russia symbol ethnic diversity russian federation traditional dress russia symbol ethnic diversity russian federation arts culture handicrafts people traditional dress russia truly symbol ethnic diversity russian federation it particularly known world its natural conformity luminous style image stas porter russian federation commonly known russia largest country lovely planet terms territorial expansion according historians russia firstly inhabited east slavs other nomadic pastoralists after this there long history arrivals different groups nations invaders todays russia home more than 150 million people its half population considered ethnically pure russians ethnical diversity much reflected colors kinds traditional dress russia image burovvv ethnical diversity much reflected colors kinds traditional dress russia russian traditional outfits enriched splendid arrangements decorative motifs red color most popular clothing heritage russia omsk kosovorotka traditional russian shirt man which actually long sleeved garment which reaches down mid thigh image kosovorotka image copyright majorshots kosovorotka traditional russian shirt man which actually long sleeved garment which reaches down mid thigh kosovorotka normally having no buttons downwards having several buttons collar unfastened when garment pulled over wearers head though these positioned off one side regional styles vary between left right instead centrally customary typical western 20th 21st century mans shirt left unbuttoned collar appears skewed which accounts garments name collar sleeves kosovorotka often decorated traditional slavic ornament rubashka long attire originally made home spun linen image burovvv 2 rubashka long attire originally made home spun linen which use both russian men russian women hundreds years now same worn various styles decorations although we dont find perfect example traditional dress peasant russia yet rubashka somehow represents rural population russia men also wore rubashka either belt outside their pants tucked into their pants traditional russian kaftan little bit different usual mesopotamian kaftans traditional russian kaftan little bit different usual mesopotamian kaftans it usually worn most conservative sect religious believers it long suit tight sleeves which 19th century became most widely spread type outer clothing among peasants merchants russian women love wear bright colored delicately designed traditional outfits image archer10 dennis russian women love wear bright colored delicately designed outfits prepare themselves traditional appearances ceremonies weddings folk events many russian women have skillful crafting art sewing such dresses art normally transferred them via inheritance traditional dresses women russia not so complicated over whelmed their patterns designs they useful casual wearing also girls traditional costume st petersburg russia image radzfoto russian women make their blouses very charming embellished usually their blouses adorned embroidery work beads they normally select cotton fabric decorate them symmetric stylized pattern embroidery this their centuries old tradition such ancient folk motifs still fashion today sarafan long trapeze shaped traditional russian jumper dress ladies image franco foli\n",
      "\n",
      "example 3\n",
      "query where first ioc run olympics held 21st century\n",
      "answer olympic games timeline historyonthenet olympic games timeline last updated 01 17 2017 11 35 more information counter intuitive facts ancient medieval history see anthony esolens politically incorrect guide western civilization athens greece first olympics first recorded evidence ancient olympic games games held olympia there only one event mens 200m sprint 490bce greece first marathon messenger pheidippedes ran 42km sparta athens bring news greek victory battle marathon 424bce greece decline large numbers young men having go fight against spartans there fewer athletes able train compete games so they began fall into decline 394ce roman emperor theodosius i abolished games claiming they pagan event 1612 cotswolds uk cotswold olympick games robert dover barrister founded cotswold olympick games games featured horse racing fencing shin kicking throwing hammer 1766 englishman richard chandler discovered site ancient olympia 1796 france french olympic revival lolympiade de la r publique olympic style yearly competition held between 1796 1798 france 1850 shropshire uk wenlock olympian games olympic style yearly sports festival established much wenlock shropshire uk dr william penny brookes it continues this day 1859 greece zappian games dr william penny brookes persuaded greek evangelis zappas stage revival ancient olympic games brookes sent 10 pounds be used prize money 1875 german funded team archeologists excavated site ancient olympia 1890 shropshire uk pierre de coubertin visited much wenlock pierre de coubertin invited attend much wenlock games dr william penny brookes 1894 france ioc founded international olympic committee founded baron pierre de coubertin who took title president committee coubertin suggested that olympics be re established international four yearly event be hosted different countries each time 1896 athens greece first modern olympic games first modern olympic games held athens greece all winners presented olive branch silver medal 1900 paris france women competed first time paris hosted these games without stadium most events held bois de boulogne swimming events held river seine women competed first time 1904 st louis usa gold silver bronze medals introduced gold silver bronze medals introduced winners events given gold medal second place silver medal third place bronze medal 1908 london uk olympic stadium first time olympics take place purpose built stadium figure skating introduced olympic sport australia new zealand competed together australasia 1912 stockholm sweden competitors all 5 continents athletes all 5 continents take part public address system electric timing devices used first time decathlon pentathlon introduced first time 1914 pierre de coubertin designed olympic symbol 5 interlocking rings 1916 due be held berlin this games cancelled due world war one 1920 antwerp belgium doves first released olympic rings symbol used first time it depicted flag white background which has been used ever since known olympic flag opening ceremony saw doves which symbolise peace released first time austria bulgaria germany hungary turkey not allowed take part because they defeated world war one 1924 first winter olympics held chamonix france 1924 games returned paris france 1928 second winter olympics held st moritz switzerland 1928 summer olympics olympic flame first lit amsterdam had previously bid 1920 1924 games\n",
      "\n",
      "example 4\n",
      "query humans medical condition kyphosis affects which part body\n",
      "answer kyphosis causes mayo clinic mayo clinic staff individual bones vertebrae that make up healthy spine look like cylinders stacked column kyphosis occurs when vertebrae upper back become more wedge shaped this deformity can be caused variety problems including osteoporosis this bone thinning disorder can result crushed vertebrae compression fractures osteoporosis most common older adults particularly women people who have taken high doses corticosteroids long periods time disk degeneration soft circular disks act cushions between spinal vertebrae age these disks dry out shrink which often worsens kyphosis scheuermanns disease also called scheuermanns kyphosis this disease typically begins during growth spurt that occurs before puberty boys affected more often than girls rounding back may worsen child finishes growing birth defects babys spinal column doesnt develop properly womb spinal bones may not form properly causing kyphosis syndromes kyphosis children can also be associated certain syndromes such marfan syndrome prader willi disease cancer cancer treatments cancer spine can weaken vertebrae make them more prone compression fractures can chemotherapy radiation cancer treatments increased curve upper spine also can be caused slouching called postural kyphosis this condition doesnt involve any deformities spine its most common teenagers frontera wr et al essentials physical medicine rehabilitation musculoskeletal disorders pain rehabilitation 2nd ed philadelphia pa saunders elsevier 2008 http www clinicalkey com accessed july 11 2013 kyphosis roundback spine american academy orthopaedic surgeons http orthoinfo aaos org topic cfm topic a00423 accessed april 10 2012 kado dm overview hyperkyphosis older persons http www uptodate com home accessed july 11 2013 kliegman rm et al nelson textbook pediatrics 19th ed philadelphia pa saunders elsevier 2011 http www clinicalkey com accessed july 11 2013 canale st et al campbells operative orthopaedics 11th ed philadelphia pa mosby elsevier 2008 http www mdconsult com das book body 208746819 4 0 1584 0 html accessed july 11 2013 neurological diagnostic tests procedures national institute neurological disorders stroke http www ninds nih gov disorders misc diagnostic tests htm accessed july 11 2013 larson expert opinion mayo clinic rochester minn dec 9 2013 wybier m et al musculoskeletal imaging progress eos imaging system joint bone spine 2013 80 238 six years more than 60 lifesized anatomic models help plan complex surgeries mayo clinic http newsletters mayo edu newscenter article aspx contentid docman 0000157800 accessed jan 7 2014 golden ak decision support system mayo clinic rochester minn nov 5 2013 products services\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('example', i)\n",
    "    print('query', dataset['train'][i]['query'])\n",
    "    print('answer', dataset['train'][i]['answer'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extracting answers\n",
    "\n",
    "Because the answers in our dataset are not unique, we will extract them and create a separate dataset containing only the unique answers. We will do this for each split separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_answers(subset):\n",
    "    \"\"\"\n",
    "    Extracts unique answers from the subset of the dataset and builds a dictionary with answers as keys and ids as values.\n",
    "    Args:\n",
    "        subset: a subset of the dataset\n",
    "\n",
    "    Returns: a dictionary mapping answers to their ids\n",
    "    \"\"\"\n",
    "    answer_to_id = {}\n",
    "    answers = list(set(subset['answer']))\n",
    "    for i, answer in enumerate(answers):\n",
    "        answer_to_id[answer] = i\n",
    "    return answer_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply this function separately to each subset and create the answers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'answer'],\n",
      "        num_rows: 48023\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'answer'],\n",
      "        num_rows: 9738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'answer'],\n",
      "        num_rows: 9720\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_answer_to_id = get_answers(dataset['train'])\n",
    "valid_answer_to_id = get_answers(dataset['validation'])\n",
    "test_answer_to_id = get_answers(dataset['test'])\n",
    "\n",
    "answers_dataset = DatasetDict({\n",
    "    'train': datasets.Dataset.from_dict({'id': range(len(train_answer_to_id)), 'answer': train_answer_to_id.keys()}),\n",
    "    'validation': datasets.Dataset.from_dict(\n",
    "        {'id': range(len(valid_answer_to_id)), 'answer': valid_answer_to_id.keys()}),\n",
    "    'test': datasets.Dataset.from_dict({'id': range(len(test_answer_to_id)), 'answer': test_answer_to_id.keys()})\n",
    "})\n",
    "print(answers_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The last thing we will have to do is to connect the answers in the original dataset to the ids of answers (in the answers dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise 2: Setting answer ids\n",
    "Fill in the following function to find and set the `answer_id` field with the id of the answer. The function accepts one of the `answer_to_id` dictionaries that you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_answer_id(example, answer_to_id):\n",
    "    \"\"\"\n",
    "    Sets the answer_id field in the example based on the answer_to_id dictionary\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        answer_to_id: a dictionary mapping answers to their ids\n",
    "\n",
    "    Returns: the updated example with the 'answer_id' field\n",
    "    \"\"\"\n",
    "    answer = example['answer']\n",
    "    example['answer_id'] = answer_to_id[answer]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we apply the function to each split separately making sure to pass the correct `answer_to_id` dictionary. We also remove the `answer` columns from the original dataset, as now we can reference the correct answer through the `answer_id` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9199e1dc1f4a5a87f22730c8c9af88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting ids for answers (train):   0%|          | 0/53346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793acfdb683c488994610c6f91c0c5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting ids for answers (validation):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008d900365594ed193fb829c98ef5dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting ids for answers (test:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(set_answer_id,\n",
    "                                        fn_kwargs={'answer_to_id': train_answer_to_id},\n",
    "                                        desc=\"Setting ids for answers (train)\")\n",
    "dataset['validation'] = dataset['validation'].map(set_answer_id,\n",
    "                                                  fn_kwargs={'answer_to_id': valid_answer_to_id},\n",
    "                                                  desc=\"Setting ids for answers (validation)\")\n",
    "dataset['test'] = dataset['test'].map(set_answer_id,\n",
    "                                      fn_kwargs={'answer_to_id': test_answer_to_id},\n",
    "                                      desc=\"Setting ids for answers (test\")\n",
    "\n",
    "dataset = dataset.remove_columns('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizing\n",
    "\n",
    "<a name='e3'></a>\n",
    "#### Exercise 3: Tokenizing\n",
    "As always, we will need to tokenize the dataset in order to create bat-of-words and TF-IDF representations in the next sections. You can use the function from the previous lab or use a library such as [Natural Language Toolkit (NLTK) library]([https://www.nltk.org/]) (https://www.nltk.org/). Complete the following function to split the text into tokens.\n",
    "\n",
    "Contrary to the previous lab, we will not include the special tokens (unknown, beginning, and end of the sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text that is assumed to be cleaned first with the clean() function. The tokenized sequence should start with the `bos_token` token and end with the 'eos_token'.\n",
    "    Args:\n",
    "        text: a cleaned text\n",
    "\n",
    "    Returns: tokenized text as a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = None  # list of tokens, your code should fill this variable\n",
    "    import nltk\n",
    "    tokens = nltk.tokenize.wordpunct_tokenize(text)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply your function to both the `query` field in the original dataset and `answer` field in the answers dataset. We save the tokenized queries in `query_tokens` field and answers in `answer_tokens` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ebae87bfbb4dcb8d2f02470fcdd405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing queries:   0%|          | 0/53346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85070bb50e134b4d819c81bedcf30c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing queries:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbbb83237c543f3b03c8e60ea29f450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing queries:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer_id', 'query_tokens'],\n",
      "        num_rows: 53346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'answer_id', 'query_tokens'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'answer_id', 'query_tokens'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e234ad163a4e01bfbad6e6217a1067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing answers:   0%|          | 0/48023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e8be8d4f7f4370ba63ddf4e7b2ccb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing answers:   0%|          | 0/9738 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943ffcdae7614a48b744ce8769b530eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing answers:   0%|          | 0/9720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers_dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'answer', 'answer_tokens'],\n",
      "        num_rows: 48023\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'answer', 'answer_tokens'],\n",
      "        num_rows: 9738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'answer', 'answer_tokens'],\n",
      "        num_rows: 9720\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_example(example, src_column, tgt_column):\n",
    "    \"\"\"\n",
    "    Applies the tokenize() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'query_tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    query = example[src_column]\n",
    "    example[tgt_column] = tokenize(query)\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_example,\n",
    "                      fn_kwargs={'src_column': 'query', 'tgt_column': 'query_tokens'},\n",
    "                      desc=\"Tokenizing queries\")\n",
    "print('dataset')\n",
    "print(dataset)\n",
    "\n",
    "answers_dataset = answers_dataset.map(tokenize_example,\n",
    "                                      fn_kwargs={'src_column': 'answer', 'tgt_column': 'answer_tokens'},\n",
    "                                      desc=\"Tokenizing answers\")\n",
    "print('answers_dataset')\n",
    "print(answers_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine some examples of tokenized queries and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: 0\n",
      "query: deriving turkish word maidservant what french name given female slave harem\n",
      "query_tokens: ['deriving', 'turkish', 'word', 'maidservant', 'what', 'french', 'name', 'given', 'female', 'slave', 'harem']\n",
      "answer_id: 33260\n",
      "answer: online etymology dictionary occupy v mid 14c take possession also take up space time employ someone irregularly borrowed old french occuper occupy person place hold seize 13c directly latin occupare take over seize take into possession possess occupy ob over see ob intensive form capere grasp seize pie root kap grasp see capable final syllable english word difficult explain it old record perhaps modification made anglo french during 16c 17c common euphemism have sexual intercourse sense attested early 15c which caused it fall polite usage captaine gods light these villaines wil make word odious word occupy which excellent good worde before it il sorted doll tearsheet 2 henry iv occur v 1520s meet meet argument middle french occurrer happen unexpectedly directly latin occurrere run meet run against befall present itself ob against toward see ob currere run see current adj sense development meet present itself appear happen present itself course events meaning come into ones mind 1620s related occurred occurring ocean n late 13c old french occean ocean 12c modern french oc latin oceanus greek okeanos great river sea surrounding disk earth opposed mediterranean unknown origin personified oceanus son uranus gaia husband tethys early times when only known land masses eurasia africa ocean endless river that flowed around them until c 1650 commonly ocean sea translating latin mare oceanum application individual bodies water began 14c there usually reckoned be five them this arbitrary also occasionally applied smaller subdivisions such german ocean north sea oceania southern pacific island australia conceived continent 1849 modern latin french oc anie c 1812 apparently coined danish geographer conrad malte brun 1755 1826 earlier english oceanica 1832 oceania name one superstates orwells nineteen eighty four oceanea name james harringtons 17c ideal state later applied british empire ochlocracy n government rabble 1580s french ochlocratie 1560s greek okhlokratia polybius mob rule lowest grade democracy kratos rule power strength see cracy okhlos mob populace perhaps literally moving mass pie wogh lo root wegh go transport vehicle see weigh sense development compare mob n related ochlocratic ochlocratical greek also had okhlagogos mob leader ochlagogue ochre n type clayey soil much used pigments late 14c old french ocre c 1300 directly late latin ocra latin ochra greek ochra ochros pale yellow unknown origin color name brownish yellow it attested mid 15c related ochreous octa before vowels oct word forming element meaning eight greek okta okt pie okto u eight see eight variant form octo often appears words taken latin greek form said be more common english octave n c 1300 utaves plural via anglo french popular old french form oitieve otaves reformed early 15c medieval latin octava latin octava dies eighth day fem octavus eighth octo see eight originally period eight days after festival also eighth day after festival counting both days inclusive reckoning thus festival sunday octaves would be following sunday verse sense stanza eight lines 1580s musical sense note eight diatonic degrees above below give\n",
      "answer_tokens: ['online', 'etymology', 'dictionary', 'occupy', 'v', 'mid', '14c', 'take', 'possession', 'also', 'take', 'up', 'space', 'time', 'employ', 'someone', 'irregularly', 'borrowed', 'old', 'french', 'occuper', 'occupy', 'person', 'place', 'hold', 'seize', '13c', 'directly', 'latin', 'occupare', 'take', 'over', 'seize', 'take', 'into', 'possession', 'possess', 'occupy', 'ob', 'over', 'see', 'ob', 'intensive', 'form', 'capere', 'grasp', 'seize', 'pie', 'root', 'kap', 'grasp', 'see', 'capable', 'final', 'syllable', 'english', 'word', 'difficult', 'explain', 'it', 'old', 'record', 'perhaps', 'modification', 'made', 'anglo', 'french', 'during', '16c', '17c', 'common', 'euphemism', 'have', 'sexual', 'intercourse', 'sense', 'attested', 'early', '15c', 'which', 'caused', 'it', 'fall', 'polite', 'usage', 'captaine', 'gods', 'light', 'these', 'villaines', 'wil', 'make', 'word', 'odious', 'word', 'occupy', 'which', 'excellent', 'good', 'worde', 'before', 'it', 'il', 'sorted', 'doll', 'tearsheet', '2', 'henry', 'iv', 'occur', 'v', '1520s', 'meet', 'meet', 'argument', 'middle', 'french', 'occurrer', 'happen', 'unexpectedly', 'directly', 'latin', 'occurrere', 'run', 'meet', 'run', 'against', 'befall', 'present', 'itself', 'ob', 'against', 'toward', 'see', 'ob', 'currere', 'run', 'see', 'current', 'adj', 'sense', 'development', 'meet', 'present', 'itself', 'appear', 'happen', 'present', 'itself', 'course', 'events', 'meaning', 'come', 'into', 'ones', 'mind', '1620s', 'related', 'occurred', 'occurring', 'ocean', 'n', 'late', '13c', 'old', 'french', 'occean', 'ocean', '12c', 'modern', 'french', 'oc', 'latin', 'oceanus', 'greek', 'okeanos', 'great', 'river', 'sea', 'surrounding', 'disk', 'earth', 'opposed', 'mediterranean', 'unknown', 'origin', 'personified', 'oceanus', 'son', 'uranus', 'gaia', 'husband', 'tethys', 'early', 'times', 'when', 'only', 'known', 'land', 'masses', 'eurasia', 'africa', 'ocean', 'endless', 'river', 'that', 'flowed', 'around', 'them', 'until', 'c', '1650', 'commonly', 'ocean', 'sea', 'translating', 'latin', 'mare', 'oceanum', 'application', 'individual', 'bodies', 'water', 'began', '14c', 'there', 'usually', 'reckoned', 'be', 'five', 'them', 'this', 'arbitrary', 'also', 'occasionally', 'applied', 'smaller', 'subdivisions', 'such', 'german', 'ocean', 'north', 'sea', 'oceania', 'southern', 'pacific', 'island', 'australia', 'conceived', 'continent', '1849', 'modern', 'latin', 'french', 'oc', 'anie', 'c', '1812', 'apparently', 'coined', 'danish', 'geographer', 'conrad', 'malte', 'brun', '1755', '1826', 'earlier', 'english', 'oceanica', '1832', 'oceania', 'name', 'one', 'superstates', 'orwells', 'nineteen', 'eighty', 'four', 'oceanea', 'name', 'james', 'harringtons', '17c', 'ideal', 'state', 'later', 'applied', 'british', 'empire', 'ochlocracy', 'n', 'government', 'rabble', '1580s', 'french', 'ochlocratie', '1560s', 'greek', 'okhlokratia', 'polybius', 'mob', 'rule', 'lowest', 'grade', 'democracy', 'kratos', 'rule', 'power', 'strength', 'see', 'cracy', 'okhlos', 'mob', 'populace', 'perhaps', 'literally', 'moving', 'mass', 'pie', 'wogh', 'lo', 'root', 'wegh', 'go', 'transport', 'vehicle', 'see', 'weigh', 'sense', 'development', 'compare', 'mob', 'n', 'related', 'ochlocratic', 'ochlocratical', 'greek', 'also', 'had', 'okhlagogos', 'mob', 'leader', 'ochlagogue', 'ochre', 'n', 'type', 'clayey', 'soil', 'much', 'used', 'pigments', 'late', '14c', 'old', 'french', 'ocre', 'c', '1300', 'directly', 'late', 'latin', 'ocra', 'latin', 'ochra', 'greek', 'ochra', 'ochros', 'pale', 'yellow', 'unknown', 'origin', 'color', 'name', 'brownish', 'yellow', 'it', 'attested', 'mid', '15c', 'related', 'ochreous', 'octa', 'before', 'vowels', 'oct', 'word', 'forming', 'element', 'meaning', 'eight', 'greek', 'okta', 'okt', 'pie', 'okto', 'u', 'eight', 'see', 'eight', 'variant', 'form', 'octo', 'often', 'appears', 'words', 'taken', 'latin', 'greek', 'form', 'said', 'be', 'more', 'common', 'english', 'octave', 'n', 'c', '1300', 'utaves', 'plural', 'via', 'anglo', 'french', 'popular', 'old', 'french', 'form', 'oitieve', 'otaves', 'reformed', 'early', '15c', 'medieval', 'latin', 'octava', 'latin', 'octava', 'dies', 'eighth', 'day', 'fem', 'octavus', 'eighth', 'octo', 'see', 'eight', 'originally', 'period', 'eight', 'days', 'after', 'festival', 'also', 'eighth', 'day', 'after', 'festival', 'counting', 'both', 'days', 'inclusive', 'reckoning', 'thus', 'festival', 'sunday', 'octaves', 'would', 'be', 'following', 'sunday', 'verse', 'sense', 'stanza', 'eight', 'lines', '1580s', 'musical', 'sense', 'note', 'eight', 'diatonic', 'degrees', 'above', 'below', 'give']\n",
      "\n",
      "example: 1\n",
      "query: what typical subject paintings dgas\n",
      "query_tokens: ['what', 'typical', 'subject', 'paintings', 'dgas']\n",
      "answer_id: 7554\n",
      "answer: edgar degas 1834 1917 painting drawing essay heilbrunn timeline art history metropolitan museum art metropolitan museum art edgar degas 1834 1917 painting drawing see works art works art 20 essay edgar degas seems never have reconciled himself label impressionist preferring call himself realist independent nevertheless he one groups founders organizer its exhibitions one its most important core members like impressionists he sought capture fleeting moments flow modern life yet he showed little interest painting plein air landscapes favoring scenes theaters caf s illuminated artificial light which he used clarify contours his figures adhering his academic training degas born 1834 scion wealthy banking family educated classics including latin greek ancient history lyc e louis le grand paris his father recognized his sons artistic gifts early encouraged his efforts drawing taking him frequently paris museums degas began copying italian renaissance paintings louvre trained studio louis lamothe who taught traditional academic style its emphasis line its insistence crucial importance draftsmanship degas also strongly influenced paintings frescoes he saw during several long trips italy late 1850s he made many sketches drawings them his notebooks evidence degass classical education can be seen his relatively static frieze like early painting young spartans exercising ca 1860 national gallery london done while he still his twenties yet despite title suggestion classical drapery some figures background there little that places subject this painting ancient greece indeed it has been noted that young girls have snub noses immature bodies montmartre types forerunners dancers degas painted so often throughout his career after 1865 when salon accepted his history painting misfortunes city orl ans mus e dorsay paris degas did not paint academic subjects again focusing his attention scenes modern life he began paint scenes such urban leisure activities horse racing after about 1870 caf concert singers ballet dancers degass choice subject matter reflects his modern approach he favored scenes ballet dancers laundresses milliners milliners 1882 29 100 38 denizens parisian low life his interest ballet dancers intensified 1870s eventually he produced approximately 1 500 works subject these not traditional portraits studies that address movement human body exploring physicality discipline dancers through use contorted postures unexpected vantage points dancer adjusting her slipper 1873 29 100 941 figures pose difficult decipher viewed steep angle both her feet her head bottom picture yet it conveys sense dancers flexibility degas absorbed artistic tradition outside influences reinterpreted them innovative ways following opening trade japan 1854 many french artists including degas increasingly influenced japanese prints whereas his contemporaries often infused their paintings eastern imagery degas abstracted these prints their inventive compositions points view particularly his use cropping asymmetry degas had also observed how sixteenth century italian mannerists similarly framed their subjects sometimes cutting off part figure example woman seated beside vase flowers 1865 29 100 128 figure cut off right edge painting part her left hand just barely visible lower right corner\n",
      "answer_tokens: ['edgar', 'degas', '1834', '1917', 'painting', 'drawing', 'essay', 'heilbrunn', 'timeline', 'art', 'history', 'metropolitan', 'museum', 'art', 'metropolitan', 'museum', 'art', 'edgar', 'degas', '1834', '1917', 'painting', 'drawing', 'see', 'works', 'art', 'works', 'art', '20', 'essay', 'edgar', 'degas', 'seems', 'never', 'have', 'reconciled', 'himself', 'label', 'impressionist', 'preferring', 'call', 'himself', 'realist', 'independent', 'nevertheless', 'he', 'one', 'groups', 'founders', 'organizer', 'its', 'exhibitions', 'one', 'its', 'most', 'important', 'core', 'members', 'like', 'impressionists', 'he', 'sought', 'capture', 'fleeting', 'moments', 'flow', 'modern', 'life', 'yet', 'he', 'showed', 'little', 'interest', 'painting', 'plein', 'air', 'landscapes', 'favoring', 'scenes', 'theaters', 'caf', 's', 'illuminated', 'artificial', 'light', 'which', 'he', 'used', 'clarify', 'contours', 'his', 'figures', 'adhering', 'his', 'academic', 'training', 'degas', 'born', '1834', 'scion', 'wealthy', 'banking', 'family', 'educated', 'classics', 'including', 'latin', 'greek', 'ancient', 'history', 'lyc', 'e', 'louis', 'le', 'grand', 'paris', 'his', 'father', 'recognized', 'his', 'sons', 'artistic', 'gifts', 'early', 'encouraged', 'his', 'efforts', 'drawing', 'taking', 'him', 'frequently', 'paris', 'museums', 'degas', 'began', 'copying', 'italian', 'renaissance', 'paintings', 'louvre', 'trained', 'studio', 'louis', 'lamothe', 'who', 'taught', 'traditional', 'academic', 'style', 'its', 'emphasis', 'line', 'its', 'insistence', 'crucial', 'importance', 'draftsmanship', 'degas', 'also', 'strongly', 'influenced', 'paintings', 'frescoes', 'he', 'saw', 'during', 'several', 'long', 'trips', 'italy', 'late', '1850s', 'he', 'made', 'many', 'sketches', 'drawings', 'them', 'his', 'notebooks', 'evidence', 'degass', 'classical', 'education', 'can', 'be', 'seen', 'his', 'relatively', 'static', 'frieze', 'like', 'early', 'painting', 'young', 'spartans', 'exercising', 'ca', '1860', 'national', 'gallery', 'london', 'done', 'while', 'he', 'still', 'his', 'twenties', 'yet', 'despite', 'title', 'suggestion', 'classical', 'drapery', 'some', 'figures', 'background', 'there', 'little', 'that', 'places', 'subject', 'this', 'painting', 'ancient', 'greece', 'indeed', 'it', 'has', 'been', 'noted', 'that', 'young', 'girls', 'have', 'snub', 'noses', 'immature', 'bodies', 'montmartre', 'types', 'forerunners', 'dancers', 'degas', 'painted', 'so', 'often', 'throughout', 'his', 'career', 'after', '1865', 'when', 'salon', 'accepted', 'his', 'history', 'painting', 'misfortunes', 'city', 'orl', 'ans', 'mus', 'e', 'dorsay', 'paris', 'degas', 'did', 'not', 'paint', 'academic', 'subjects', 'again', 'focusing', 'his', 'attention', 'scenes', 'modern', 'life', 'he', 'began', 'paint', 'scenes', 'such', 'urban', 'leisure', 'activities', 'horse', 'racing', 'after', 'about', '1870', 'caf', 'concert', 'singers', 'ballet', 'dancers', 'degass', 'choice', 'subject', 'matter', 'reflects', 'his', 'modern', 'approach', 'he', 'favored', 'scenes', 'ballet', 'dancers', 'laundresses', 'milliners', 'milliners', '1882', '29', '100', '38', 'denizens', 'parisian', 'low', 'life', 'his', 'interest', 'ballet', 'dancers', 'intensified', '1870s', 'eventually', 'he', 'produced', 'approximately', '1', '500', 'works', 'subject', 'these', 'not', 'traditional', 'portraits', 'studies', 'that', 'address', 'movement', 'human', 'body', 'exploring', 'physicality', 'discipline', 'dancers', 'through', 'use', 'contorted', 'postures', 'unexpected', 'vantage', 'points', 'dancer', 'adjusting', 'her', 'slipper', '1873', '29', '100', '941', 'figures', 'pose', 'difficult', 'decipher', 'viewed', 'steep', 'angle', 'both', 'her', 'feet', 'her', 'head', 'bottom', 'picture', 'yet', 'it', 'conveys', 'sense', 'dancers', 'flexibility', 'degas', 'absorbed', 'artistic', 'tradition', 'outside', 'influences', 'reinterpreted', 'them', 'innovative', 'ways', 'following', 'opening', 'trade', 'japan', '1854', 'many', 'french', 'artists', 'including', 'degas', 'increasingly', 'influenced', 'japanese', 'prints', 'whereas', 'his', 'contemporaries', 'often', 'infused', 'their', 'paintings', 'eastern', 'imagery', 'degas', 'abstracted', 'these', 'prints', 'their', 'inventive', 'compositions', 'points', 'view', 'particularly', 'his', 'use', 'cropping', 'asymmetry', 'degas', 'had', 'also', 'observed', 'how', 'sixteenth', 'century', 'italian', 'mannerists', 'similarly', 'framed', 'their', 'subjects', 'sometimes', 'cutting', 'off', 'part', 'figure', 'example', 'woman', 'seated', 'beside', 'vase', 'flowers', '1865', '29', '100', '128', 'figure', 'cut', 'off', 'right', 'edge', 'painting', 'part', 'her', 'left', 'hand', 'just', 'barely', 'visible', 'lower', 'right', 'corner']\n",
      "\n",
      "example: 2\n",
      "query: which part body would russian valenki normally be worn\n",
      "query_tokens: ['which', 'part', 'body', 'would', 'russian', 'valenki', 'normally', 'be', 'worn']\n",
      "answer_id: 21250\n",
      "answer: traditional dress russia symbol ethnic diversity russian federation traditional dress russia symbol ethnic diversity russian federation arts culture handicrafts people traditional dress russia truly symbol ethnic diversity russian federation it particularly known world its natural conformity luminous style image stas porter russian federation commonly known russia largest country lovely planet terms territorial expansion according historians russia firstly inhabited east slavs other nomadic pastoralists after this there long history arrivals different groups nations invaders todays russia home more than 150 million people its half population considered ethnically pure russians ethnical diversity much reflected colors kinds traditional dress russia image burovvv ethnical diversity much reflected colors kinds traditional dress russia russian traditional outfits enriched splendid arrangements decorative motifs red color most popular clothing heritage russia omsk kosovorotka traditional russian shirt man which actually long sleeved garment which reaches down mid thigh image kosovorotka image copyright majorshots kosovorotka traditional russian shirt man which actually long sleeved garment which reaches down mid thigh kosovorotka normally having no buttons downwards having several buttons collar unfastened when garment pulled over wearers head though these positioned off one side regional styles vary between left right instead centrally customary typical western 20th 21st century mans shirt left unbuttoned collar appears skewed which accounts garments name collar sleeves kosovorotka often decorated traditional slavic ornament rubashka long attire originally made home spun linen image burovvv 2 rubashka long attire originally made home spun linen which use both russian men russian women hundreds years now same worn various styles decorations although we dont find perfect example traditional dress peasant russia yet rubashka somehow represents rural population russia men also wore rubashka either belt outside their pants tucked into their pants traditional russian kaftan little bit different usual mesopotamian kaftans traditional russian kaftan little bit different usual mesopotamian kaftans it usually worn most conservative sect religious believers it long suit tight sleeves which 19th century became most widely spread type outer clothing among peasants merchants russian women love wear bright colored delicately designed traditional outfits image archer10 dennis russian women love wear bright colored delicately designed outfits prepare themselves traditional appearances ceremonies weddings folk events many russian women have skillful crafting art sewing such dresses art normally transferred them via inheritance traditional dresses women russia not so complicated over whelmed their patterns designs they useful casual wearing also girls traditional costume st petersburg russia image radzfoto russian women make their blouses very charming embellished usually their blouses adorned embroidery work beads they normally select cotton fabric decorate them symmetric stylized pattern embroidery this their centuries old tradition such ancient folk motifs still fashion today sarafan long trapeze shaped traditional russian jumper dress ladies image franco foli\n",
      "answer_tokens: ['traditional', 'dress', 'russia', 'symbol', 'ethnic', 'diversity', 'russian', 'federation', 'traditional', 'dress', 'russia', 'symbol', 'ethnic', 'diversity', 'russian', 'federation', 'arts', 'culture', 'handicrafts', 'people', 'traditional', 'dress', 'russia', 'truly', 'symbol', 'ethnic', 'diversity', 'russian', 'federation', 'it', 'particularly', 'known', 'world', 'its', 'natural', 'conformity', 'luminous', 'style', 'image', 'stas', 'porter', 'russian', 'federation', 'commonly', 'known', 'russia', 'largest', 'country', 'lovely', 'planet', 'terms', 'territorial', 'expansion', 'according', 'historians', 'russia', 'firstly', 'inhabited', 'east', 'slavs', 'other', 'nomadic', 'pastoralists', 'after', 'this', 'there', 'long', 'history', 'arrivals', 'different', 'groups', 'nations', 'invaders', 'todays', 'russia', 'home', 'more', 'than', '150', 'million', 'people', 'its', 'half', 'population', 'considered', 'ethnically', 'pure', 'russians', 'ethnical', 'diversity', 'much', 'reflected', 'colors', 'kinds', 'traditional', 'dress', 'russia', 'image', 'burovvv', 'ethnical', 'diversity', 'much', 'reflected', 'colors', 'kinds', 'traditional', 'dress', 'russia', 'russian', 'traditional', 'outfits', 'enriched', 'splendid', 'arrangements', 'decorative', 'motifs', 'red', 'color', 'most', 'popular', 'clothing', 'heritage', 'russia', 'omsk', 'kosovorotka', 'traditional', 'russian', 'shirt', 'man', 'which', 'actually', 'long', 'sleeved', 'garment', 'which', 'reaches', 'down', 'mid', 'thigh', 'image', 'kosovorotka', 'image', 'copyright', 'majorshots', 'kosovorotka', 'traditional', 'russian', 'shirt', 'man', 'which', 'actually', 'long', 'sleeved', 'garment', 'which', 'reaches', 'down', 'mid', 'thigh', 'kosovorotka', 'normally', 'having', 'no', 'buttons', 'downwards', 'having', 'several', 'buttons', 'collar', 'unfastened', 'when', 'garment', 'pulled', 'over', 'wearers', 'head', 'though', 'these', 'positioned', 'off', 'one', 'side', 'regional', 'styles', 'vary', 'between', 'left', 'right', 'instead', 'centrally', 'customary', 'typical', 'western', '20th', '21st', 'century', 'mans', 'shirt', 'left', 'unbuttoned', 'collar', 'appears', 'skewed', 'which', 'accounts', 'garments', 'name', 'collar', 'sleeves', 'kosovorotka', 'often', 'decorated', 'traditional', 'slavic', 'ornament', 'rubashka', 'long', 'attire', 'originally', 'made', 'home', 'spun', 'linen', 'image', 'burovvv', '2', 'rubashka', 'long', 'attire', 'originally', 'made', 'home', 'spun', 'linen', 'which', 'use', 'both', 'russian', 'men', 'russian', 'women', 'hundreds', 'years', 'now', 'same', 'worn', 'various', 'styles', 'decorations', 'although', 'we', 'dont', 'find', 'perfect', 'example', 'traditional', 'dress', 'peasant', 'russia', 'yet', 'rubashka', 'somehow', 'represents', 'rural', 'population', 'russia', 'men', 'also', 'wore', 'rubashka', 'either', 'belt', 'outside', 'their', 'pants', 'tucked', 'into', 'their', 'pants', 'traditional', 'russian', 'kaftan', 'little', 'bit', 'different', 'usual', 'mesopotamian', 'kaftans', 'traditional', 'russian', 'kaftan', 'little', 'bit', 'different', 'usual', 'mesopotamian', 'kaftans', 'it', 'usually', 'worn', 'most', 'conservative', 'sect', 'religious', 'believers', 'it', 'long', 'suit', 'tight', 'sleeves', 'which', '19th', 'century', 'became', 'most', 'widely', 'spread', 'type', 'outer', 'clothing', 'among', 'peasants', 'merchants', 'russian', 'women', 'love', 'wear', 'bright', 'colored', 'delicately', 'designed', 'traditional', 'outfits', 'image', 'archer10', 'dennis', 'russian', 'women', 'love', 'wear', 'bright', 'colored', 'delicately', 'designed', 'outfits', 'prepare', 'themselves', 'traditional', 'appearances', 'ceremonies', 'weddings', 'folk', 'events', 'many', 'russian', 'women', 'have', 'skillful', 'crafting', 'art', 'sewing', 'such', 'dresses', 'art', 'normally', 'transferred', 'them', 'via', 'inheritance', 'traditional', 'dresses', 'women', 'russia', 'not', 'so', 'complicated', 'over', 'whelmed', 'their', 'patterns', 'designs', 'they', 'useful', 'casual', 'wearing', 'also', 'girls', 'traditional', 'costume', 'st', 'petersburg', 'russia', 'image', 'radzfoto', 'russian', 'women', 'make', 'their', 'blouses', 'very', 'charming', 'embellished', 'usually', 'their', 'blouses', 'adorned', 'embroidery', 'work', 'beads', 'they', 'normally', 'select', 'cotton', 'fabric', 'decorate', 'them', 'symmetric', 'stylized', 'pattern', 'embroidery', 'this', 'their', 'centuries', 'old', 'tradition', 'such', 'ancient', 'folk', 'motifs', 'still', 'fashion', 'today', 'sarafan', 'long', 'trapeze', 'shaped', 'traditional', 'russian', 'jumper', 'dress', 'ladies', 'image', 'franco', 'foli']\n",
      "\n",
      "example: 3\n",
      "query: where first ioc run olympics held 21st century\n",
      "query_tokens: ['where', 'first', 'ioc', 'run', 'olympics', 'held', '21st', 'century']\n",
      "answer_id: 15424\n",
      "answer: olympic games timeline historyonthenet olympic games timeline last updated 01 17 2017 11 35 more information counter intuitive facts ancient medieval history see anthony esolens politically incorrect guide western civilization athens greece first olympics first recorded evidence ancient olympic games games held olympia there only one event mens 200m sprint 490bce greece first marathon messenger pheidippedes ran 42km sparta athens bring news greek victory battle marathon 424bce greece decline large numbers young men having go fight against spartans there fewer athletes able train compete games so they began fall into decline 394ce roman emperor theodosius i abolished games claiming they pagan event 1612 cotswolds uk cotswold olympick games robert dover barrister founded cotswold olympick games games featured horse racing fencing shin kicking throwing hammer 1766 englishman richard chandler discovered site ancient olympia 1796 france french olympic revival lolympiade de la r publique olympic style yearly competition held between 1796 1798 france 1850 shropshire uk wenlock olympian games olympic style yearly sports festival established much wenlock shropshire uk dr william penny brookes it continues this day 1859 greece zappian games dr william penny brookes persuaded greek evangelis zappas stage revival ancient olympic games brookes sent 10 pounds be used prize money 1875 german funded team archeologists excavated site ancient olympia 1890 shropshire uk pierre de coubertin visited much wenlock pierre de coubertin invited attend much wenlock games dr william penny brookes 1894 france ioc founded international olympic committee founded baron pierre de coubertin who took title president committee coubertin suggested that olympics be re established international four yearly event be hosted different countries each time 1896 athens greece first modern olympic games first modern olympic games held athens greece all winners presented olive branch silver medal 1900 paris france women competed first time paris hosted these games without stadium most events held bois de boulogne swimming events held river seine women competed first time 1904 st louis usa gold silver bronze medals introduced gold silver bronze medals introduced winners events given gold medal second place silver medal third place bronze medal 1908 london uk olympic stadium first time olympics take place purpose built stadium figure skating introduced olympic sport australia new zealand competed together australasia 1912 stockholm sweden competitors all 5 continents athletes all 5 continents take part public address system electric timing devices used first time decathlon pentathlon introduced first time 1914 pierre de coubertin designed olympic symbol 5 interlocking rings 1916 due be held berlin this games cancelled due world war one 1920 antwerp belgium doves first released olympic rings symbol used first time it depicted flag white background which has been used ever since known olympic flag opening ceremony saw doves which symbolise peace released first time austria bulgaria germany hungary turkey not allowed take part because they defeated world war one 1924 first winter olympics held chamonix france 1924 games returned paris france 1928 second winter olympics held st moritz switzerland 1928 summer olympics olympic flame first lit amsterdam had previously bid 1920 1924 games\n",
      "answer_tokens: ['olympic', 'games', 'timeline', 'historyonthenet', 'olympic', 'games', 'timeline', 'last', 'updated', '01', '17', '2017', '11', '35', 'more', 'information', 'counter', 'intuitive', 'facts', 'ancient', 'medieval', 'history', 'see', 'anthony', 'esolens', 'politically', 'incorrect', 'guide', 'western', 'civilization', 'athens', 'greece', 'first', 'olympics', 'first', 'recorded', 'evidence', 'ancient', 'olympic', 'games', 'games', 'held', 'olympia', 'there', 'only', 'one', 'event', 'mens', '200m', 'sprint', '490bce', 'greece', 'first', 'marathon', 'messenger', 'pheidippedes', 'ran', '42km', 'sparta', 'athens', 'bring', 'news', 'greek', 'victory', 'battle', 'marathon', '424bce', 'greece', 'decline', 'large', 'numbers', 'young', 'men', 'having', 'go', 'fight', 'against', 'spartans', 'there', 'fewer', 'athletes', 'able', 'train', 'compete', 'games', 'so', 'they', 'began', 'fall', 'into', 'decline', '394ce', 'roman', 'emperor', 'theodosius', 'i', 'abolished', 'games', 'claiming', 'they', 'pagan', 'event', '1612', 'cotswolds', 'uk', 'cotswold', 'olympick', 'games', 'robert', 'dover', 'barrister', 'founded', 'cotswold', 'olympick', 'games', 'games', 'featured', 'horse', 'racing', 'fencing', 'shin', 'kicking', 'throwing', 'hammer', '1766', 'englishman', 'richard', 'chandler', 'discovered', 'site', 'ancient', 'olympia', '1796', 'france', 'french', 'olympic', 'revival', 'lolympiade', 'de', 'la', 'r', 'publique', 'olympic', 'style', 'yearly', 'competition', 'held', 'between', '1796', '1798', 'france', '1850', 'shropshire', 'uk', 'wenlock', 'olympian', 'games', 'olympic', 'style', 'yearly', 'sports', 'festival', 'established', 'much', 'wenlock', 'shropshire', 'uk', 'dr', 'william', 'penny', 'brookes', 'it', 'continues', 'this', 'day', '1859', 'greece', 'zappian', 'games', 'dr', 'william', 'penny', 'brookes', 'persuaded', 'greek', 'evangelis', 'zappas', 'stage', 'revival', 'ancient', 'olympic', 'games', 'brookes', 'sent', '10', 'pounds', 'be', 'used', 'prize', 'money', '1875', 'german', 'funded', 'team', 'archeologists', 'excavated', 'site', 'ancient', 'olympia', '1890', 'shropshire', 'uk', 'pierre', 'de', 'coubertin', 'visited', 'much', 'wenlock', 'pierre', 'de', 'coubertin', 'invited', 'attend', 'much', 'wenlock', 'games', 'dr', 'william', 'penny', 'brookes', '1894', 'france', 'ioc', 'founded', 'international', 'olympic', 'committee', 'founded', 'baron', 'pierre', 'de', 'coubertin', 'who', 'took', 'title', 'president', 'committee', 'coubertin', 'suggested', 'that', 'olympics', 'be', 're', 'established', 'international', 'four', 'yearly', 'event', 'be', 'hosted', 'different', 'countries', 'each', 'time', '1896', 'athens', 'greece', 'first', 'modern', 'olympic', 'games', 'first', 'modern', 'olympic', 'games', 'held', 'athens', 'greece', 'all', 'winners', 'presented', 'olive', 'branch', 'silver', 'medal', '1900', 'paris', 'france', 'women', 'competed', 'first', 'time', 'paris', 'hosted', 'these', 'games', 'without', 'stadium', 'most', 'events', 'held', 'bois', 'de', 'boulogne', 'swimming', 'events', 'held', 'river', 'seine', 'women', 'competed', 'first', 'time', '1904', 'st', 'louis', 'usa', 'gold', 'silver', 'bronze', 'medals', 'introduced', 'gold', 'silver', 'bronze', 'medals', 'introduced', 'winners', 'events', 'given', 'gold', 'medal', 'second', 'place', 'silver', 'medal', 'third', 'place', 'bronze', 'medal', '1908', 'london', 'uk', 'olympic', 'stadium', 'first', 'time', 'olympics', 'take', 'place', 'purpose', 'built', 'stadium', 'figure', 'skating', 'introduced', 'olympic', 'sport', 'australia', 'new', 'zealand', 'competed', 'together', 'australasia', '1912', 'stockholm', 'sweden', 'competitors', 'all', '5', 'continents', 'athletes', 'all', '5', 'continents', 'take', 'part', 'public', 'address', 'system', 'electric', 'timing', 'devices', 'used', 'first', 'time', 'decathlon', 'pentathlon', 'introduced', 'first', 'time', '1914', 'pierre', 'de', 'coubertin', 'designed', 'olympic', 'symbol', '5', 'interlocking', 'rings', '1916', 'due', 'be', 'held', 'berlin', 'this', 'games', 'cancelled', 'due', 'world', 'war', 'one', '1920', 'antwerp', 'belgium', 'doves', 'first', 'released', 'olympic', 'rings', 'symbol', 'used', 'first', 'time', 'it', 'depicted', 'flag', 'white', 'background', 'which', 'has', 'been', 'used', 'ever', 'since', 'known', 'olympic', 'flag', 'opening', 'ceremony', 'saw', 'doves', 'which', 'symbolise', 'peace', 'released', 'first', 'time', 'austria', 'bulgaria', 'germany', 'hungary', 'turkey', 'not', 'allowed', 'take', 'part', 'because', 'they', 'defeated', 'world', 'war', 'one', '1924', 'first', 'winter', 'olympics', 'held', 'chamonix', 'france', '1924', 'games', 'returned', 'paris', 'france', '1928', 'second', 'winter', 'olympics', 'held', 'st', 'moritz', 'switzerland', '1928', 'summer', 'olympics', 'olympic', 'flame', 'first', 'lit', 'amsterdam', 'had', 'previously', 'bid', '1920', '1924', 'games']\n",
      "\n",
      "example: 4\n",
      "query: humans medical condition kyphosis affects which part body\n",
      "query_tokens: ['humans', 'medical', 'condition', 'kyphosis', 'affects', 'which', 'part', 'body']\n",
      "answer_id: 37397\n",
      "answer: kyphosis causes mayo clinic mayo clinic staff individual bones vertebrae that make up healthy spine look like cylinders stacked column kyphosis occurs when vertebrae upper back become more wedge shaped this deformity can be caused variety problems including osteoporosis this bone thinning disorder can result crushed vertebrae compression fractures osteoporosis most common older adults particularly women people who have taken high doses corticosteroids long periods time disk degeneration soft circular disks act cushions between spinal vertebrae age these disks dry out shrink which often worsens kyphosis scheuermanns disease also called scheuermanns kyphosis this disease typically begins during growth spurt that occurs before puberty boys affected more often than girls rounding back may worsen child finishes growing birth defects babys spinal column doesnt develop properly womb spinal bones may not form properly causing kyphosis syndromes kyphosis children can also be associated certain syndromes such marfan syndrome prader willi disease cancer cancer treatments cancer spine can weaken vertebrae make them more prone compression fractures can chemotherapy radiation cancer treatments increased curve upper spine also can be caused slouching called postural kyphosis this condition doesnt involve any deformities spine its most common teenagers frontera wr et al essentials physical medicine rehabilitation musculoskeletal disorders pain rehabilitation 2nd ed philadelphia pa saunders elsevier 2008 http www clinicalkey com accessed july 11 2013 kyphosis roundback spine american academy orthopaedic surgeons http orthoinfo aaos org topic cfm topic a00423 accessed april 10 2012 kado dm overview hyperkyphosis older persons http www uptodate com home accessed july 11 2013 kliegman rm et al nelson textbook pediatrics 19th ed philadelphia pa saunders elsevier 2011 http www clinicalkey com accessed july 11 2013 canale st et al campbells operative orthopaedics 11th ed philadelphia pa mosby elsevier 2008 http www mdconsult com das book body 208746819 4 0 1584 0 html accessed july 11 2013 neurological diagnostic tests procedures national institute neurological disorders stroke http www ninds nih gov disorders misc diagnostic tests htm accessed july 11 2013 larson expert opinion mayo clinic rochester minn dec 9 2013 wybier m et al musculoskeletal imaging progress eos imaging system joint bone spine 2013 80 238 six years more than 60 lifesized anatomic models help plan complex surgeries mayo clinic http newsletters mayo edu newscenter article aspx contentid docman 0000157800 accessed jan 7 2014 golden ak decision support system mayo clinic rochester minn nov 5 2013 products services\n",
      "answer_tokens: ['kyphosis', 'causes', 'mayo', 'clinic', 'mayo', 'clinic', 'staff', 'individual', 'bones', 'vertebrae', 'that', 'make', 'up', 'healthy', 'spine', 'look', 'like', 'cylinders', 'stacked', 'column', 'kyphosis', 'occurs', 'when', 'vertebrae', 'upper', 'back', 'become', 'more', 'wedge', 'shaped', 'this', 'deformity', 'can', 'be', 'caused', 'variety', 'problems', 'including', 'osteoporosis', 'this', 'bone', 'thinning', 'disorder', 'can', 'result', 'crushed', 'vertebrae', 'compression', 'fractures', 'osteoporosis', 'most', 'common', 'older', 'adults', 'particularly', 'women', 'people', 'who', 'have', 'taken', 'high', 'doses', 'corticosteroids', 'long', 'periods', 'time', 'disk', 'degeneration', 'soft', 'circular', 'disks', 'act', 'cushions', 'between', 'spinal', 'vertebrae', 'age', 'these', 'disks', 'dry', 'out', 'shrink', 'which', 'often', 'worsens', 'kyphosis', 'scheuermanns', 'disease', 'also', 'called', 'scheuermanns', 'kyphosis', 'this', 'disease', 'typically', 'begins', 'during', 'growth', 'spurt', 'that', 'occurs', 'before', 'puberty', 'boys', 'affected', 'more', 'often', 'than', 'girls', 'rounding', 'back', 'may', 'worsen', 'child', 'finishes', 'growing', 'birth', 'defects', 'babys', 'spinal', 'column', 'doesnt', 'develop', 'properly', 'womb', 'spinal', 'bones', 'may', 'not', 'form', 'properly', 'causing', 'kyphosis', 'syndromes', 'kyphosis', 'children', 'can', 'also', 'be', 'associated', 'certain', 'syndromes', 'such', 'marfan', 'syndrome', 'prader', 'willi', 'disease', 'cancer', 'cancer', 'treatments', 'cancer', 'spine', 'can', 'weaken', 'vertebrae', 'make', 'them', 'more', 'prone', 'compression', 'fractures', 'can', 'chemotherapy', 'radiation', 'cancer', 'treatments', 'increased', 'curve', 'upper', 'spine', 'also', 'can', 'be', 'caused', 'slouching', 'called', 'postural', 'kyphosis', 'this', 'condition', 'doesnt', 'involve', 'any', 'deformities', 'spine', 'its', 'most', 'common', 'teenagers', 'frontera', 'wr', 'et', 'al', 'essentials', 'physical', 'medicine', 'rehabilitation', 'musculoskeletal', 'disorders', 'pain', 'rehabilitation', '2nd', 'ed', 'philadelphia', 'pa', 'saunders', 'elsevier', '2008', 'http', 'www', 'clinicalkey', 'com', 'accessed', 'july', '11', '2013', 'kyphosis', 'roundback', 'spine', 'american', 'academy', 'orthopaedic', 'surgeons', 'http', 'orthoinfo', 'aaos', 'org', 'topic', 'cfm', 'topic', 'a00423', 'accessed', 'april', '10', '2012', 'kado', 'dm', 'overview', 'hyperkyphosis', 'older', 'persons', 'http', 'www', 'uptodate', 'com', 'home', 'accessed', 'july', '11', '2013', 'kliegman', 'rm', 'et', 'al', 'nelson', 'textbook', 'pediatrics', '19th', 'ed', 'philadelphia', 'pa', 'saunders', 'elsevier', '2011', 'http', 'www', 'clinicalkey', 'com', 'accessed', 'july', '11', '2013', 'canale', 'st', 'et', 'al', 'campbells', 'operative', 'orthopaedics', '11th', 'ed', 'philadelphia', 'pa', 'mosby', 'elsevier', '2008', 'http', 'www', 'mdconsult', 'com', 'das', 'book', 'body', '208746819', '4', '0', '1584', '0', 'html', 'accessed', 'july', '11', '2013', 'neurological', 'diagnostic', 'tests', 'procedures', 'national', 'institute', 'neurological', 'disorders', 'stroke', 'http', 'www', 'ninds', 'nih', 'gov', 'disorders', 'misc', 'diagnostic', 'tests', 'htm', 'accessed', 'july', '11', '2013', 'larson', 'expert', 'opinion', 'mayo', 'clinic', 'rochester', 'minn', 'dec', '9', '2013', 'wybier', 'm', 'et', 'al', 'musculoskeletal', 'imaging', 'progress', 'eos', 'imaging', 'system', 'joint', 'bone', 'spine', '2013', '80', '238', 'six', 'years', 'more', 'than', '60', 'lifesized', 'anatomic', 'models', 'help', 'plan', 'complex', 'surgeries', 'mayo', 'clinic', 'http', 'newsletters', 'mayo', 'edu', 'newscenter', 'article', 'aspx', 'contentid', 'docman', '0000157800', 'accessed', 'jan', '7', '2014', 'golden', 'ak', 'decision', 'support', 'system', 'mayo', 'clinic', 'rochester', 'minn', 'nov', '5', '2013', 'products', 'services']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('example:', i)\n",
    "    print('query:', dataset['train'][i]['query'])\n",
    "    print('query_tokens:', dataset['train'][i]['query_tokens'])\n",
    "    answer_id = dataset['train'][i]['answer_id']\n",
    "    print('answer_id:', answer_id)\n",
    "    print('answer:', answers_dataset['train'][answer_id]['answer'])\n",
    "    print('answer_tokens:', answers_dataset['train'][answer_id]['answer_tokens'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice the difference in the types of the different structures we use. Run the following cell to check the types. Do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "--\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "--\n",
      "deriving turkish word maidservant what french name given female slave harem\n",
      "<class 'str'>\n",
      "--\n",
      "['deriving', 'turkish', 'word', 'maidservant', 'what', 'french', 'name', 'given', 'female', 'slave', 'harem']\n",
      "<class 'list'>\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#type of original dataset\n",
    "print(type(dataset))\n",
    "print(\"--\")\n",
    "#type of the split of the dataset\n",
    "print(type(dataset['test']))\n",
    "print(\"--\")\n",
    "#type of original query\n",
    "print(dataset['train'][0]['query'])\n",
    "print(type(dataset['train'][0]['query']))\n",
    "print(\"--\")\n",
    "#type of tokenized query\n",
    "print(dataset['train'][0]['query_tokens'])\n",
    "print(type(dataset['train'][0]['query_tokens']))\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "In this section you will built a bag-of-words representation of the dataset. We will use numpy arrays to store the results. The bag-of-words representation is a simple and effective way to represent text data. It involves creating a vocabulary of unique words from the dataset and representing each sentence as a vector of word counts. We first need the vocabulary, which we will build from both the full sentences and the compressed sentences. Similar to the first lab, the vocabulary will be a list of unique words from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extracting Vocabulary\n",
    "\n",
    "<a name='e4'></a>\n",
    "#### Exercise 4: Extracting vocabulary counts\n",
    "\n",
    "In the following cell, you will implement a function that takes two datasets (`dataset`, and `answers_dataset`) and returns a dictionary with the counts of each word in the vocabulary. The dictionary should be of the form {word: count}. As in previous lab, you will use the `Counter` class from the `collections` module to do this. Iterate over the two datasets and count the tokens in `query_tokens` and `answer_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary_counts(dataset, answers_dataset):\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from the tokenized sentences\n",
    "    Args:\n",
    "        dataset: a Dataset from which 'query_tokens' are used to build vocabulary\n",
    "        answers_dataset: a Dataset from which 'answer_tokens' are used to build vocabulary\n",
    "\n",
    "    Returns: a Counter object with the counts of each word in the vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = Counter()\n",
    "    \n",
    "    # count words from tokenized queries\n",
    "    for example in dataset:\n",
    "        vocab.update(example['query_tokens'])\n",
    "\n",
    "    # count words from tokenized answers\n",
    "    for example in answers_dataset:\n",
    "        vocab.update(example['answer_tokens'])\n",
    "    \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we use the function you implemented. Notice that we build our vocabulary based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322032\n",
      "[('that', 175639), ('it', 149839), ('his', 141680), ('he', 130170), ('this', 107533), ('which', 94308), ('be', 88796), ('you', 71244), ('one', 69319), ('have', 68932)]\n"
     ]
    }
   ],
   "source": [
    "vocab_counter = extract_vocabulary_counts(dataset['train'], answers_dataset['train'])\n",
    "print(len(vocab_counter))\n",
    "print(vocab_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will truncate the vocabulary. We also create the handy `token_to_id` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 20_000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "token_to_id = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "\n",
    "<a name='e5'></a>\n",
    "#### Exercise 5: Bag of Words\n",
    "Here we will create the bag-of-words representation of the sentences. The function will take a single sentence (list of tokens) and return an array of size `vocab_size` with the counts of each word in the vocabulary. The\n",
    "`vocab_size` is calculated as the length of the passed `token_to_id` dictionary. The resulting array should have zeros everywhere but the indices corresponding to the words in the vocabulary where it should have the counts of the words in the sentence. For example, if the sentence is `['fox', 'and', 'deer']` and the vocabulary is `{'fox': 0, 'and': 1, 'deer': 2}`, the resulting array should be `[1, 1, 1]`. If the sentence is `['fox', 'and', 'fox', 'deer']`, the resulting array should be `[2, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(sentence_tokens, token_to_id):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of the sentence\n",
    "    Args:\n",
    "        sentence_tokens: a list of tokens\n",
    "        token_to_id: a dictionary mapping each word to an index in the vocabulary\n",
    "\n",
    "    Returns:: a numpy array of size vocab_size with the counts of each word in the vocabulary\n",
    "    \"\"\"\n",
    "    vocab_size = len(token_to_id)\n",
    "    bow = np.zeros(vocab_size, dtype=int)\n",
    "\n",
    "    for token in sentence_tokens:\n",
    "        if token in token_to_id:\n",
    "            token_id = token_to_id[token]\n",
    "            bow[token_id] += 1\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the function. The output should be a numpy array of size `vocab_size` with the counts of each word in the vocabulary. Notice that most of the elements of the BOW representation are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:\n",
      "['what', 'does', 'vertebral', 'column', 'protect']\n",
      "Bag of words:\n",
      "[0 0 0 ... 0 0 0]\n",
      "Type of bag of words:\n",
      "<class 'numpy.ndarray'>\n",
      "Shape of bag of words:\n",
      "(20000,)\n",
      "Non-zero elements in bag of words:\n",
      "[   23   224  2117  3668 18274]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sentence:')\n",
    "print(dataset['test'][0]['query_tokens'])\n",
    "query_bow = bag_of_words(dataset['test'][0]['query_tokens'], token_to_id)\n",
    "query_non_zero_bow = np.nonzero(query_bow)[0]\n",
    "\n",
    "print('Bag of words:')\n",
    "print(query_bow)\n",
    "print('Type of bag of words:')\n",
    "print(type(query_bow))\n",
    "print('Shape of bag of words:')\n",
    "print(query_bow.shape)\n",
    "print('Non-zero elements in bag of words:')\n",
    "print(query_non_zero_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine further the non-zero elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero elements in bag of words:\n",
      "[   23   224  2117  3668 18274]\n",
      "what : 1\n",
      "does : 1\n",
      "protect : 1\n",
      "column : 1\n",
      "vertebral : 1\n"
     ]
    }
   ],
   "source": [
    "print('Non-zero elements in bag of words:')\n",
    "print(query_non_zero_bow)\n",
    "for i in query_non_zero_bow:\n",
    "    print(vocab[i], ':', query_bow[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Function for Embedding Text\n",
    "\n",
    "The following function will apply all the steps we implemented to a single sentence. It returns a bag of words representation that we will use to calculate the similarity between different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "[   4  463 3309]\n"
     ]
    }
   ],
   "source": [
    "def embed_text(text, clean_fn, tokenize_fn, embed_fn):\n",
    "    \"\"\"\n",
    "    Embeds the text using the provided functions. The pipeline applies cleaning (clean_fn), tokenization (tokenize_fn), and embedding (embed_fn).\n",
    "    Args:\n",
    "        text: the text to be embedded\n",
    "        clean_fn: function/Callable clean_fn(text:str):str\n",
    "        tokenize_fn: function/Callable tokenize_fn(text:str): List[str]\n",
    "        embed_fn: function/Callable embed_fn(tokens:List[str]): np.ndarray\n",
    "\n",
    "    Returns: the embedding of the text as a numpy array\n",
    "    \"\"\"\n",
    "    cleaned = clean_fn(text)\n",
    "    tokens = tokenize_fn(cleaned)\n",
    "    embedding = embed_fn(tokens)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "embedding = embed_text(\"This is an example of a sentence\", clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "print(embedding.shape)\n",
    "print(np.nonzero(embedding)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "<a name='e6'></a>\n",
    "#### Exercise 6: Cosine Similarity between two vectors\n",
    "\n",
    "Complete the following function that given any two vectors will compute the cosine similarity. If you don't remember the formula for the cosine similarity, revisit the course material. Notice that the function receives numpy arrays and recall that you can express cosine similarity as a dot product. Use numpy functions to write an efficient implementation. Two more exercises builds upon this one, so make sure to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    Args:\n",
    "        vector1: numpy array of the first vector\n",
    "        vector2: numpy array of the second vector\n",
    "\n",
    "    Returns: cosine similarity\n",
    "\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm1 = np.linalg.norm(vector1)\n",
    "    norm2 = np.linalg.norm(vector2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9999999999999998)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.array([0, 1, 2]), np.array([0, 2, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how similar are the BOW representations of some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: fox and deer\n",
      "Cosine Similarity: 0.2673 - Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Cosine Similarity: 0.0000 - Sentence: Some interesting document containing sentences.\n",
      "Cosine Similarity: 0.2236 - Sentence: The quick brown fox jumps over the lazy cat and some other stuff.\n",
      "Cosine Similarity: 0.7071 - Sentence: Fox and deer are not friends.\n",
      "Cosine Similarity: 0.3086 - Sentence: Fox and deer are not friends. But this document is a lot longer than the previous one. We can add sentence by sentence and see how the embeddings change.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'Some interesting document containing sentences.',\n",
    "    'The quick brown fox jumps over the lazy cat and some other stuff.',\n",
    "    'Fox and deer are not friends.',\n",
    "    'Fox and deer are not friends. But this document is a lot longer than the previous one. We can add sentence by sentence and see how the embeddings change.',\n",
    "]\n",
    "embedded_sentences = [\n",
    "    embed_text(sentence, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "query = 'fox and deer'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "cosine_similarities = [\n",
    "    cosine_similarity(embedded_query, embedded_sentence)\n",
    "    for embedded_sentence in embedded_sentences\n",
    "]\n",
    "print(f'Query: {query}')\n",
    "for sent, cos_sim in zip(sentences, cosine_similarities):\n",
    "    print(f'Cosine Similarity: {cos_sim:.4f} - Sentence: {sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "In this section, we will use the BOW representations to finally search for the answers to our questions. We start by calculating the BOWs of queries and answers of the whole `validation` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 20124.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9738/9738 [00:03<00:00, 2586.51it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_queries_bows = []\n",
    "for example in tqdm.tqdm(dataset['validation']):\n",
    "    valid_queries_bows.append(bag_of_words(example['query_tokens'], token_to_id))\n",
    "\n",
    "valid_answers_bows = []\n",
    "for example in tqdm.tqdm(answers_dataset['validation']):\n",
    "    valid_answers_bows.append(bag_of_words(example['answer_tokens'], token_to_id))\n",
    "\n",
    "valid_queries_bows = np.array(valid_queries_bows)\n",
    "valid_answers_bows = np.array(valid_answers_bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e7'></a>\n",
    "#### Exercise 7: Cosine Similarity between a vector and an array of vectors\n",
    "\n",
    "The next step in our retrieval system, would be to calculate the proximity of a query to our retrieval corpus (in our case that is all the sentences).\n",
    "\n",
    "Complete the following function to calculate the cosine similarity between a vector (first parameter `vector`, that will usually be the query vector) and all other vectors (second parameter `other_vectors`, that will be the sentence embeddings in our case). Note that the `other_vectors` parameter is a single numpy array of size `N x D`, where $N$ is the number of vectors and $D$ is the dimension of each vector.\n",
    "\n",
    "For maximum efficiency (we will need it) do not use loops. Try to write the implementation with numpy functions. Hint: matrix multiplication can be seen as calculating the dot product between rows and columns of the multiplied matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_1_to_n(vector, other_vectors):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a single vector and other vectors.\n",
    "    Args:\n",
    "        vector: a numpy array representing a vector of D dimensions\n",
    "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
    "\n",
    "    Returns: a 1D numpy array of size N containing the cosine similarity between the vector and all the other vectors\n",
    "\n",
    "    \"\"\"\n",
    "    dot_products = other_vectors @ vector\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    other_norms = np.linalg.norm(other_vectors, axis=1)\n",
    "    denominator = vector_norm * other_norms\n",
    "\n",
    "    similarities = np.divide(\n",
    "        dot_products,\n",
    "        denominator,\n",
    "        out=np.zeros_like(dot_products, dtype=float),\n",
    "        where=denominator != 0\n",
    "    )\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now can try out our retrieval system by calculating the cosine similarities between the query and all answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9738,)\n",
      "[0.         0.03026138 0.03553345 0.07846532 0.05619636 0.02398006\n",
      " 0.02080288 0.03125382 0.0071606  0.05863313]\n"
     ]
    }
   ],
   "source": [
    "query = 'Which vegetable is Blackadder‚Äôs servant obsessed with in the UK television series ‚ÄòBlackadder II‚Äô?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "print(query_similarity.shape)\n",
    "print(query_similarity[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4973\n",
      "0.3422987432310777\n",
      "[0 0 7 ... 0 0 0]\n",
      "blackadder third blackadder third title screen blackadder third created 17 september 1987 22 october 1987 chronology website blackadder third 1 third series bbc sitcom blackadder written richard curtis ben elton which aired 17 september 22 october 1987 series set during british regency saw principal character mr e blackadder serve butler prince regent have contend cash fads age embraced his master third series reduced number principal characters again compared previous series instead included number significant cameo roles well known comic actors 2 programme won bafta award best comedy series 1988 received three further nominations 3 contents 8 external links plot blackadder third set late 18th early 19th centuries period known regency much this time king george iii incapacitated due poor mental health his son george prince wales acted regent during this period he known prince regent although regency place between 1811 1820 historical events persons depicted referenced appear date series before this time anywhere period age enlightenment between 1755 publication samuel johnsons dictionary 1805 just before battle trafalgar series e blackadder esquire rowan atkinson head butler prince wales hugh laurie spoiled foppish idiot despite edmunds respected intelligence abilities he has no personal fortune speak other hand given ease which he able manipulate prince he generally good financial straits according edmund he has been serving prince regent all his life ever since prince breastfed when he had show prince which part his mother serving drinks baldrick tony robinson remains similar his blackadder ii predecessor although his cunning plans cease be even remotely intelligent except last episode he most aware political religious social events blackadder himself now servant baldrick labelled blackadders dogsbody this series baldrick often displays more belligerent attitude towards his master even referring him once lazy big nosed rubber faced bastard blackadder often affectionately calls him balders baldrick sometimes calls blackadder mr b there three main sets princes quarters which large lavish below stairs kitchen hangout blackadder baldrick which dark squalid though fairness very large very high ceiling finally mrs miggins coffeehouse mrs miggins pie shop never seen running gag blackadder ii she least descendant hers now finally shown played helen atkinson wood plots series feature number then contemporary issues personalities such rotten boroughs dr samuel johnson played robbie coltrane french revolution featuring chris barrie scarlet pimpernel over top theatrical actors squirrel hating female highwaymen practice settling quarrels duel discussing tactics duke wellington played stephen fry last episode series also features rowan atkinson role blackadders scottish cousin macadder supposedly fierce swordsman this leads dialogue which atkinson acting both parts following aftermath this episode blackadder finds fortune ends up permanently posing prince regent after real prince regent disguised blackadder shot duke wellington episodes series aired six episodes broadcast thursdays 9 30 bbc one titles episodes always noun paired another derived\n"
     ]
    }
   ],
   "source": [
    "most_similar = int(np.argmax(query_similarity))\n",
    "print(most_similar)\n",
    "print(query_similarity[most_similar])\n",
    "print(valid_answers_bows[most_similar])\n",
    "print(answers_dataset['validation'][most_similar]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function returns the indices of the top-k elements in the array. If the `sorted` parameter is `True` (it is by default) the returned array will be sorted in the descending order (of the corresponding values in array). For example, if the `array` is `[3, 2, 4, 1]` and `k=2` the returned numpy array will be `[2, 0]` if `sorted` is True (the top values are `3` and `4` with indices `0` and `2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_k_indices(array, k, sorted=True):\n",
    "    \"\"\"\n",
    "    Returns top-k indices from the 1D array. If `sorted` is `True` the returned indices are sorted in the descending order\n",
    "    Args:\n",
    "        array: a 1D numpy array\n",
    "        k: a number of top indices to return\n",
    "        sorted: if True, the returned indices are sorted in descending order\n",
    "\n",
    "    Returns: a 1D numpy array containing top-k indices\n",
    "\n",
    "    \"\"\"\n",
    "    top_k = np.argpartition(array, -k)[-k:]\n",
    "    if sorted:\n",
    "        selected = array[top_k]\n",
    "        sorted_selected = (-selected).argsort()\n",
    "        top_k = top_k[sorted_selected]\n",
    "    return top_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackadder third blackadder third title screen blackadder third created 17 september 1987 22 october 1987 chronology website blackadder third 1 third series bbc sitcom blackadder written richard curtis ben elton which aired 17 september 22 october 1987 series set during british regency saw principal character mr e blackadder serve butler prince regent have contend cash fads age embraced his master third series reduced number principal characters again compared previous series instead included number significant cameo roles well known comic actors 2 programme won bafta award best comedy series 1988 received three further nominations 3 contents 8 external links plot blackadder third set late 18th early 19th centuries period known regency much this time king george iii incapacitated due poor mental health his son george prince wales acted regent during this period he known prince regent although regency place between 1811 1820 historical events persons depicted referenced appear date series before this time anywhere period age enlightenment between 1755 publication samuel johnsons dictionary 1805 just before battle trafalgar series e blackadder esquire rowan atkinson head butler prince wales hugh laurie spoiled foppish idiot despite edmunds respected intelligence abilities he has no personal fortune speak other hand given ease which he able manipulate prince he generally good financial straits according edmund he has been serving prince regent all his life ever since prince breastfed when he had show prince which part his mother serving drinks baldrick tony robinson remains similar his blackadder ii predecessor although his cunning plans cease be even remotely intelligent except last episode he most aware political religious social events blackadder himself now servant baldrick labelled blackadders dogsbody this series baldrick often displays more belligerent attitude towards his master even referring him once lazy big nosed rubber faced bastard blackadder often affectionately calls him balders baldrick sometimes calls blackadder mr b there three main sets princes quarters which large lavish below stairs kitchen hangout blackadder baldrick which dark squalid though fairness very large very high ceiling finally mrs miggins coffeehouse mrs miggins pie shop never seen running gag blackadder ii she least descendant hers now finally shown played helen atkinson wood plots series feature number then contemporary issues personalities such rotten boroughs dr samuel johnson played robbie coltrane french revolution featuring chris barrie scarlet pimpernel over top theatrical actors squirrel hating female highwaymen practice settling quarrels duel discussing tactics duke wellington played stephen fry last episode series also features rowan atkinson role blackadders scottish cousin macadder supposedly fierce swordsman this leads dialogue which atkinson acting both parts following aftermath this episode blackadder finds fortune ends up permanently posing prince regent after real prince regent disguised blackadder shot duke wellington episodes series aired six episodes broadcast thursdays 9 30 bbc one titles episodes always noun paired another derived\n",
      "similarity: 0.3422987432310777\n",
      "\n",
      "blackadder series tv tropes alternate history most noticeably black adder which depicts henry tudor losing battle bosworth field richard iv ruling next 13 years before eventual henry vii rewrites history books scrub out richard ivs reign downplayed blackadder ii blackadder third which does mostly follow real path history albeit humorous spin things however two major differences real history that elizabeth i soon be george iv both got killed replaced prince ludwig mr blackadder respectively blackadder cavalier years most part follows lead second third series putting humorous spin english civil war execution charles i ends implying that baby that real life became charles ii after restoration will end up being killed thanks blackadders treachery presumably meaning that blackadder must have found peasant baby replace him averted blackadder goes forth which only two exceptions manfred von richtoven field marshall haig deals entirely fictional characters events within larger setting world war i sole difference between events show real world history would be that von richtoven got shot apparently killed flashheart 1917 rather than getting killed when his plane shot down 1918 turned up eleven blackadder back forth which gives us two alternate histories one after blackadders first trip through time which french conquered uk 19th century one after blackadder knows that he can change present which blackadder dynasty has been power centuries artistic license history many many examples per episode say nothing shows overall track record hey rule funny people plus black adder can explain away its inaccuracies henry tudor doing lousy job rewriting history stretch you could say that prince ludwig elizabeth i blackadder george iv did something similar second third series eliminating all artifacts 13 year reign would be difficult trick say least one reasons we know extremely obscure roman emperor elagabalus who declared damnatio memoriae whose name expunged thoroughly official histories empire because coinage their face name it survives present day elagabalus reigned mere three years very last episode fourth series averted viewers know that world war i ended 1918 so when capt darling thinks war has finally ended mentioning year 1917 it becomes clear that characters doomed bad guy wins first series starts bad guy henry tudor having effectively won already although he loses battle bosworth field first episode he eventually ends up claiming throne thirteen years later after percy accidentally poisons royal family death then real kicker he rewrites history books erase richard ivs reign altogether blackadder third ends most ruthless evil blackadders usurping identity prince regent blackadder goes forth ends all main cast members falling victim madness modern war real villain this instalment melchetts questionable strategies contrast other series ending isnt played laughs blackadder back forth had modern incarnation blackadder manipulate history via time travel become king united kingdom making baldrick his puppet prime minister blackadder cavalier years ends blackadder defecting roundheads ratting out both baldrick that baby that real life grew up be charles ii blackadders christmas carol may very well be most extreme example it ends uncharacteristically kind hearted ebenezer blackadd\n",
      "similarity: 0.2938824268298956\n",
      "\n",
      "macclesfield pub quiz league november 2015 macclesfield pub quiz league set park tavern brewers q1 great britain appear tennis davis cup final which belgian city ghent q2 once storms abigail barney clodagh desmond eva have passed uk which will be next frank current ebola outbreak started which african country guinea dec 2013 where would you find connexus versatile tv program apprentice teams names current tv series q5 which actor has appeared james bond exactly 2 official bond films timothy dalton living daylights license kill q6 according collins english dictionary what has been chosen word year 2015 binge watch q7 what tag line upcoming star wars film episode 7 series force awakens who replaced nick hewer tv program apprentice claude littner q9 which raf base news october owing arrival 140 migrants boat raf akrotiri cyprus q10 there one remaining hovercraft service operating uk which city does it operate portsmouth southsea ryde isle wight q11 baroness dido harding winscombe has been news recently ceo which company talk talk q12 which british airline celebrating its 20th anniversary flying its inaugural flight november 10th 1995 easyjet schengen treaty takes its name village which country luxembourg mp can stand two things ordnance survey map name either mile post mooring post q15 marine map what does hwm stand high water mark which country host next winter olympics 2018 south korea who did seb coe succeed head iaaf lamine diack what third largest object solar system saturn sun jupiter saturn which man made object furthest earth voyager 1 allow voyager q20 his part which 1953 film did frank sinatra receive best supporting actor oscar here eternity which current world leader sometimes known bibi benjamin netanyahu q22 who has been recently sworn canadas 23rd prime minister after winning surprise majority justin trudeau what longest motorway uk m6 what longest road uk a1 who shadow chancellor john mcdonnell which building built 1093 house shrine st cuthbert durham cathedral which building would you find famous cosmati pavement westminster cathedral who hosts modern life goodish dave gorman frankie fredericks represented which african country athletics namibia who hosts yet untitled alan davies who will be new host qi succeeding stephen fry sandi toksvig what word used describe animal plant that both male female hermaphrodite which artistic medium would you associate ansel adams photography which city normally accepted being ancient capital wessex winchester which group recorded track unfinished symphony massive attack which school featured uk tvs please sir fenn street q37 80s band heaven 17 got their name well known novel originally published 1962 name it clockwork orange anthony burgess q38 steely dan got their name which notorious novel originally published 1959 naked lunch william burroughs q39 wladimir klitschko champion boxer which country ukraine rockhampton rocket nickname given which famous sportsman rod laver which british astronaut going international space station december tim peake how many cantons make up switzerland 26 accept 25 27 q43 which city imperial capital japan before tokyo kyoto saloth sar born 19 may 1925 better known what name pol pot what discovered 1799 pierre fran ois bouchard napoleonic soldier rosetta stone i told you i ill words carved into whose gravestone spike milligan q47 what did newcastle chemist william owen invent 1927 those\n",
      "similarity: 0.2807017543859649\n",
      "\n",
      "ben elton biography ben elton biography ben elton born may 3 1959 british comedian writer born catford london immigrant family academics he studied godalming grammar school became stand up comedian comedy writer shortly after leaving manchester university 1980 he central figure alternative comedy scene early 1980s 1980 he wrote appeared granada televisions sketch show alfresco which also notable early appearances stephen fry hugh laurie emma thompson robbie coltrane which received poor ratings his first major success co writer television sitcom young ones this followed happy families six part series starring jennifer saunders adrian edmondson then filthy rich catflap which assumed many be sequel young ones he went co write second subsequent series blackadder starring rowan atkinson tony robinson while simultaneously becoming well known his television appearances host channel 4s saturday live later moved renamed friday night live which seen uk version saturday night live many his catchphrases became well known example double seat double seat chant tired commuters wanting space themselves train home he graduated his closing slot first series saturday live hosting show 1990 he starred his own stand up comedy sketch series entitled man auntie which had second series 1994 similar format used ben elton show which aired 1998 his most recent television sitcom thin blue line set police station also starring atkinson which ran two series 1995 1996 prolific ambitious he began writing novels plays including stark 1989 made into television series which elton starred gridlock 1991 this other eden 1993 popcorn 1996 blast past 1998 inconceivable latter which based authors own experiences vitro fertilisation ivf treatment made into film under title maybe baby 2000 directed elton himself more recently he has embarked career musical theatre co writing beautiful game andrew lloyd webber then rock musicals we will rock you music queen tonights night music rod stewart his latest novels dead famous 2001 variation classical whodunnit 1930s 1940s set around reality tv series akin big brother high society 2002 novel exploring social consequences drug illegality britain he currently lives notting hill london his wife sophie gare saxophonist all girl band called boom babies\n",
      "similarity: 0.27718891832383846\n",
      "\n",
      "museum broadcast communications encyclopedia television cagney lacey cagney lacey u s police series cagney lacey u s police procedural pervasive melodramatic overtones deservedly one most widely discussed programs television history series aired cbs television network 1982 88 presented set bold dramatic combinations blending bending genre character narrative strategies though rated list top 25 programs only once during those years show drew critical acclaim controversy established substantial audience fiercely loyal viewers who least one occasion helped save program cancellation network demonstrated television scholar julie daccis outstanding study defining women television case cagney lacey history cagney lacey provides textbook case illustrating many issues pervasive u s television industry well that industrys complicated relationship social cultural issues created its earliest version writer producers barbara corday barbara avedon 1974 cagney lacey first designed feature film unable sell project women presented it television networks potential series rebuffed again they finally brought cagney lacey screen 1981 made television movie co produced barney rosenzweig then cordays husband movie drew high ratings led series which premiered 1982 difficulties involved production history this point indicate struggles encountered women writers producers film television industries especially when their work focuses women those difficulties however merely beginning continuing contests put dacci negotiation meanings women woman femininity took place among variety vested interests considerable conflict throughout run series negotiations continued interests included creative team series producers writers actors directors they also included network executives officials every level television critics special interest groups unusually involved audience that actively participated ongoing discussions series meanings directions while many these controversies took place sets writers meetings board rooms one earliest spilled over into public discussion newspapers magazines letters made television movie character christine cagney played loretta swit that mary beth lacey tyne daly unavailable take cagney role series because her continuing work m s h swit replaced meg foster almost immediately discussion cbs some public venues focused potential homosexual overtones relationship between two women foster who had played lesbian earlier television role cited masculine aggressive after considerable argument cbs threatened cancel series made fosters removal replacement condition continuing show fall 1982 season began sharon gless presumably more conventionally feminine heterosexual portraying cagney similar though not so visible conflicts adjustments continued throughout history series questions appearance dress body weight hair styles constantly under consideration negotiation story material particularly when focused issues vital concern women rape incest abortion breast cancer often proved controversial led continuing battles network standards practices offices daly reported that even matter sexual relations her fictional husband harvey john karlin differences opinion flared into ar\n",
      "similarity: 0.2649987664311087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "for idx in top_indices:\n",
    "    print(answers_dataset['validation'][idx]['answer'])\n",
    "    print(f'similarity: {query_similarity[idx]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e8'></a>\n",
    "#### Exercise 8: Analyzing and improving BOW search results\n",
    "\n",
    "Experiment with different queries (taking into account the nature of the dataset and your insights from the analysis so far).\n",
    "Answer the following questions:\n",
    "- Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 3 from each category). Provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling, ...)\n",
    "- If you see problems with search, how could you improve your implementation? Change the functions above, if you think there is room for improvement. Describe your changes and how they made the search better or (in case you made no changes) explain what made the search robust enough to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected from first 6 validation queries\n",
      "good examples: 3\n",
      "bad examples: 3\n",
      "\n",
      "good examples correct in top-5\n",
      "\n",
      "idx: 1\n",
      "query: mate mat yerba mate chimarr o popular south american originating\n",
      "correct id: 9658\n",
      "top1 id: 9658 | score: 0.5372\n",
      "top1 answer: introduction chimarr o brazilian yerba mate circle drink introduction chimarr o brazilian yerba mate dave mate february 13 2015 chimarr o videos what chimarr o brazilian yerba mate\n",
      "\n",
      "idx: 4\n",
      "query: born 1899 died 1947 cardiac arrest indicted tax evasion 1931 who he\n",
      "correct id: 1033\n",
      "top1 id: 8841 | score: 0.3105\n",
      "top1 answer: music famousdead list famous dead people category music prince born 1958 06 07 died 2016 04 21 prince rogers nelson better known simply prince one point artist formerly known princ\n",
      "\n",
      "idx: 5\n",
      "query: which bront sisters wrote novel tenant wildfell hall\n",
      "correct id: 5061\n",
      "top1 id: 5061 | score: 0.4572\n",
      "top1 answer: tenant wildfell hall anne bronte google books tenant wildfell hall 2 reviews https books google com books about tenant wildfell hall html id i0mewx yx9wc most controversial bronte \n",
      "\n",
      "bad example correct not in top-5\n",
      "\n",
      "idx: 0\n",
      "query: greek mythology how many fates there\n",
      "correct id: 9042\n",
      "top1 id: 7859 | score: 0.3838\n",
      "top1 answer: greek religion 4 britannica com greek religion religious beliefs practices ancient hellenes displaying 201 300 342 results lectisternium latin lectum sternere spread couch ancient \n",
      "\n",
      "idx: 2\n",
      "query: what louisville ky track home kentucky derby annual race known fastest two minutes sports\n",
      "correct id: 6554\n",
      "top1 id: 5783 | score: 0.3704\n",
      "top1 answer: 2017 kentucky derby online horse betting what channel kentucky derby watch kentucky derby live tv nbc 5 00 pm est horse racing kentucky rich history dating back 1789 when first rac\n",
      "\n",
      "idx: 3\n",
      "query: what name josiah wedgwoods factory near hanley\n",
      "correct id: 6745\n",
      "top1 id: 8791 | score: 0.4854\n",
      "top1 answer: gisel questions1 pastebin com gisel questions1 what name dr seusss egg hatching elephant horton who clark kents high school sweetheart lana lang what first published sherlock holme\n",
      "\n",
      " baseline vs binary bow with same queries\n",
      "\n",
      "idx: 1 | baseline is_in_top5: 1 | binary is_in_top5: 0\n",
      "\n",
      "idx: 4 | baseline is_in_top5: 1 | binary is_in_top5: 1\n",
      "\n",
      "idx: 5 | baseline is_in_top5: 1 | binary is_in_top5: 1\n",
      "\n",
      "idx: 0 | baseline is_in_top5: 0 | binary is_in_top5: 0\n",
      "\n",
      "idx: 2 | baseline is_in_top5: 0 | binary is_in_top5: 1\n",
      "\n",
      "idx: 3 | baseline is_in_top5: 0 | binary is_in_top5: 0\n",
      "\n",
      "Summary on selected queries:\n",
      "baseline is_in_top5: 3/6\n",
      "binary is_in_top5: 3/6\n"
     ]
    }
   ],
   "source": [
    "good_examples = []\n",
    "bad_examples = []\n",
    "max_queries = 150\n",
    "\n",
    "for i in range(min(max_queries, len(dataset['validation']))):\n",
    "    query = dataset['validation'][i]['query']\n",
    "    correct_answer_id = dataset['validation'][i]['answer_id']\n",
    "\n",
    "    query_bow = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "    similarities = cosine_similarity_1_to_n(query_bow, valid_answers_bows)\n",
    "    top5 = top_k_indices(similarities, k=5)\n",
    "\n",
    "    example = {\n",
    "        'idx': i,\n",
    "        'query': query,\n",
    "        'correct_answer_id': correct_answer_id,\n",
    "        'correct_answer': answers_dataset['validation'][correct_answer_id]['answer'],\n",
    "        'top1_id': int(top5[0]),\n",
    "        'top1_score': float(similarities[top5[0]]),\n",
    "        'top1_answer': answers_dataset['validation'][int(top5[0])]['answer'],\n",
    "        'hit_top5': int(correct_answer_id in top5),\n",
    "    }\n",
    "\n",
    "    if example['hit_top5'] == 1 and len(good_examples) < 3:\n",
    "        good_examples.append(example)\n",
    "    if example['hit_top5'] == 0 and len(bad_examples) < 3:\n",
    "        bad_examples.append(example)\n",
    "\n",
    "    if len(good_examples) == 3 and len(bad_examples) == 3:\n",
    "        break\n",
    "\n",
    "print('Collected from first', i + 1, 'validation queries')\n",
    "print('good examples:', len(good_examples))\n",
    "print('bad examples:', len(bad_examples))\n",
    "\n",
    "print('\\ngood examples correct in top-5')\n",
    "for ex in good_examples:\n",
    "    print('\\nidx:', ex['idx'])\n",
    "    print('query:', ex['query'])\n",
    "    print('correct id:', ex['correct_answer_id'])\n",
    "    print('top1 id:', ex['top1_id'], '| score:', round(ex['top1_score'], 4))\n",
    "    print('top1 answer:', ex['top1_answer'][:180])\n",
    "\n",
    "print('\\nbad example correct not in top-5')\n",
    "for ex in bad_examples:\n",
    "    print('\\nidx:', ex['idx'])\n",
    "    print('query:', ex['query'])\n",
    "    print('correct id:', ex['correct_answer_id'])\n",
    "    print('top1 id:', ex['top1_id'], '| score:', round(ex['top1_score'], 4))\n",
    "    print('top1 answer:', ex['top1_answer'][:180])\n",
    "\n",
    "\n",
    "valid_answers_bows_binary = (valid_answers_bows > 0).astype(np.int32)\n",
    "selected_ids = [ex['idx'] for ex in good_examples + bad_examples]\n",
    "baseline_hits = 0\n",
    "binary_hits = 0\n",
    "\n",
    "print('\\n baseline vs binary bow with same queries')\n",
    "for idx in selected_ids:\n",
    "    query = dataset['validation'][idx]['query']\n",
    "    correct_answer_id = dataset['validation'][idx]['answer_id']\n",
    "\n",
    "    query_bow = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "    sims_base = cosine_similarity_1_to_n(query_bow, valid_answers_bows)\n",
    "    top5_base = top_k_indices(sims_base, k=5)\n",
    "    hit_base = int(correct_answer_id in top5_base)\n",
    "    baseline_hits += hit_base\n",
    "\n",
    "    query_bow_binary = (query_bow > 0).astype(np.int32)\n",
    "    sims_binary = cosine_similarity_1_to_n(query_bow_binary, valid_answers_bows_binary)\n",
    "    top5_binary = top_k_indices(sims_binary, k=5)\n",
    "    hit_binary = int(correct_answer_id in top5_binary)\n",
    "    binary_hits += hit_binary\n",
    "\n",
    "    print('\\nidx:', idx, '| baseline is_in_top5:', hit_base, '| binary is_in_top5:', hit_binary)\n",
    "\n",
    "print('\\nSummary on selected queries:')\n",
    "print('baseline is_in_top5:', f\"{baseline_hits}/{len(selected_ids)}\")\n",
    "print('binary is_in_top5:', f\"{binary_hits}/{len(selected_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The BOW search works partially. On the 6 inspected validation queries, baseline performance in top_5 = 3/6\n",
    "\n",
    "- Good results (3 examples) <br>\n",
    "idx 1 : top-1 is correct. This works because the query has rare and specific words (yerba, chimarr, mate) that strongly overlap with the correct answer <br>\n",
    "idx 5 : top-1 is correct. This works because the query includes specific name entities and title words (also famous names) <br>\n",
    "idx 4 : answer is in top-5. This is partially good because lexical overlap helps retrieval, but ranking is not good (top-1 is wrong) <br>\n",
    "\n",
    "- Bad results (3 examples) <br>\n",
    "idx 0 : top-1 is a generic Greek religion text, not the exact answer. BOW matches the broad topic words but misses precise goal (‚Äúhow many fates‚Äù) <br>\n",
    "idx 2 : top-1 is a general Kentucky Derby page, not the specific location answer. BOW gets the topic overlap but not the exact relation asked <br>\n",
    "idx 3 : top-1 is unrelated noisy text. This means sensitivity to noisy and long documents and weak understanding of the semantic <br>\n",
    "\n",
    "- Improvements <br> \n",
    "We tested a binary BOW variant (with words present/absent instead of counts) <br>\n",
    "Result on the same 6 queries: baseline 3/6, binary 3/6 <br>\n",
    "Binary helped one case (idx 2) but worsened another (idx 1), so there was no overall gain on this sample <br>\n",
    "\n",
    "- Conclusion <br>\n",
    "BOW retrieval is okay for queries with different keywords, but weak for precise relations and answes with much noise <br>\n",
    "A better solution would be given by TF-IDF, which should reduce how common words impact and improves the quality of the ranking <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "In this section we will implement the TF-IDF algorithm. While BOW is a simple way to represent the documents, it has some limitations. For example, it does not take into account the importance of each word in the document. TF-IDF representation takes into account the frequency of each word in the document and the frequency of the word in the whole dataset. It is a widely used technique in information retrieval and text mining. Refer to the lecture slides for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "<a name='e9'></a>\n",
    "#### Exercise 9: Inverse Document Frequency (IDF)\n",
    "In this exercise, you will implement the TF-IDF algorithm. First, calculate Inverse Document Frequency (IDF) for each word in the vocabulary. Intuitively, it is a measure of how informative a word is based on the whole dataset. Consult the lecture slides for the details. The IDF is calculated as follows:\n",
    "$$\n",
    "IDF(t) = log_{10}(N/df(t))$$\n",
    "where $N$ is the total number of documents (sentences) in the dataset and $df(t)$ is the number of documents containing the word $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_idf(bows):\n",
    "    \"\"\"\n",
    "    Calculates the IDF for each word in the vocabulary\n",
    "    Args:\n",
    "        bows: numpty array of size (N x D) where N is the number of documents and D is the vocabulary size\n",
    "\n",
    "    Returns: a numpy array of size D with IDF values for each token\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To avoid the data leakage, the IDF should be calculated the train subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_answers_bows = []\n",
    "for example in tqdm.tqdm(answers_dataset['train']):\n",
    "    train_answers_bows.append(bag_of_words(example['answer_tokens'], token_to_id))\n",
    "\n",
    "train_answers_bows = np.array(train_answers_bows)\n",
    "\n",
    "idf = calculate_idf(train_answers_bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full TF-IDF\n",
    "\n",
    "<a name='e10'></a>\n",
    "#### Exercise 10: TF-IDF\n",
    "- Calculate TF-IDF on the `test` subset of the dataset.\n",
    "- Analyze the search results based on your implemented TF-IDF. Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 3 from each category). Provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling with average embeddings, ...)\n",
    "- Compare the results with the ones you got with the bag-of-words representation. Discuss the differences and similarities. Do you think TF-IDF is a better representation for this task? Why or why not? Provide examples to support your arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# You can implement the following functions, but you can also use your own design.\n",
    "\n",
    "def calculate_tf_idf_n(bows, idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for each word in the vocabulary\n",
    "    Args:\n",
    "        bows: numpty array of size (N x D) where N is the number of documents and D is the vocabulary size\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size (N x D) with TF-IDF values for each document and each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_tf_idf(bow, idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for a single document\n",
    "    Args:\n",
    "        bow: a numpy array of size D with the bag-of-words representation of the document\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size D with TF-IDF values for each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def embed_tf_idf(sentence, token_to_id, idf):\n",
    "    \"\"\"\n",
    "    Embeds the sentence using TF-IDF\n",
    "    Args:\n",
    "        sentence: a list of tokens\n",
    "        token_to_id: a dictionary mapping each word to an index in the vocabulary\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size D with TF-IDF values for each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# query = dataset['validation'][0]['query']\n",
    "# print('query:', query)\n",
    "# query_tfidf = embed_text(query, clean, tokenize, lambda x: embed_tf_idf(x, token_to_id, idf))\n",
    "#\n",
    "# answers_tfidf = calculate_tf_idf_n(valid_answers_bows, idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "\n",
    "Word embeddings are a powerful model for representing words and their meaning (in terms of distributional similarity). As we discussed in class, we can use them in a wide variety of tasks with more complex architectures. Word vectors offer a dense vector for each word.\n",
    "\n",
    "In this section you will load the pre-trained word embeddings model - Glove. You can read more about it [here](https://aclanthology.org/D14-1162/) ([https://aclanthology.org/D14-1162/](https://aclanthology.org/D14-1162/)). The embeddings are trained on a large corpus of text and are available in different dimensions. We will start with the dimension of 100, but later you will be asked to experiment with other dimensions.\n",
    "\n",
    "You can download the embeddings manually from one of the following links:\n",
    "- https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained/data?select=glove.6B.50d.txt\n",
    "- https://github.com/nishankmahore/word2vec-flask-api (check the table in the readme)\n",
    "\n",
    "The extracted files should contain several models but for now we will be using the 50-dimensional one. This means that each token is represented as a 50-dimensional floating point vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_embeddings_path = 'glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will load and parse this file line-by-line. Your job in the next exercise is the parsing part.\n",
    "\n",
    "<a name='e11'></a>\n",
    "#### Exercise 11: Parsing the embeddings\n",
    "Implement the following function to parse a single line of the glove embeddings file. The line contains string values separated by spaces. The first value is the token and the rest are the elements of the embedding vector (the values should be cast to float). You can inspect the file to get a better idea of what is there. Return both the token (as string) and the embedding (as a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_embeddings_row(line):\n",
    "    \"\"\"\n",
    "    Parses a single line from the GloVe embeddings file. The line contains a word followed by its embedding values separated by spaces.\n",
    "    Args:\n",
    "        line: a line from the GloVe embeddings file\n",
    "\n",
    "    Returns: a tuple (word, embedding) where 'word' is a string and 'embedding' is a numpy array of floats\n",
    "    \"\"\"\n",
    "    line_split = line.split()\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return token, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we load the file and iterate over the lines to load the tokens and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "embeddings = []\n",
    "with open(glove_embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        token, embedding = parse_embeddings_row(line)\n",
    "        tokens.append(token)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.stack(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will create a class `WordEmbeddings` that will hold embeddings and expose useful methods to embed a single token (`embed_token()`) and the whole sentence (`embed_sentence()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sentence Embeddings by Averaging Word Embeddings\n",
    "In the course, we will see different architectures that take into account the sequence of words (by combining their vectors). A first naive but simple and sometimes (as we are going to see) quite effective approach would be to represent a sentence with an embedding vector that is the average of the word vectors that form the sentence.\n",
    "\n",
    "So formally, this is what we are aiming for:\n",
    "\n",
    "$\n",
    "\\text{Sentence_Embedding} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Word_Embedding}_i\n",
    "$\n",
    "\n",
    "where:\n",
    "* $N$ is the number of words in a sentence\n",
    "* $\\text{Word_Embedding}_i$ is the word vector for the $i$-th in the sentence.\n",
    "\n",
    "Things to note:\n",
    "* The embedding vector for the sentence will obviously have the same dimension as the word embedding.\n",
    "* This representation ignores the word order (like bag-of-words). During the course we will see how we can overcome this limitation by using sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e12'></a>\n",
    "#### Exercise 12: Word Embeddings Class\n",
    "Implement the two methods in the following class:\n",
    "- `embed_token()` - accepts a token to be embedded. Use `self.token_to_id` to find the row in the `self.embeddings` attribute. If the token is outside the vocabulary, return `None`.\n",
    "- `embed_sentence()` - accepts a list of tokens that form a sentence. Each token (that is inside the vocabulary) should be embedded. The second parameter `reduction` will determine how the embedded tokens are \"reduced\". There are three options: `mean` should average the tokens (resulting in a single numpy array of size `embedding_dim`, which is 50 for our model), `sum` should sum the tokens, and `none` should return the embeddings of all tokens as a single numpy array (the array should be of shape `(N, embedding_dim)`). Think about a situation where none of the tokens in a sentence is in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordEmbeddings:\n",
    "    def __init__(self, vocabulary, embeddings):\n",
    "        \"\"\"\n",
    "        Initializes the WordEmbeddings object.\n",
    "        Args:\n",
    "            vocabulary: list of str\n",
    "            embeddings: np.ndarray of shape (vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        self.token_to_id = {token: i for i, token in enumerate(vocabulary)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(vocabulary)}\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def embed_token(self, token):\n",
    "        \"\"\"\n",
    "        Embed a single token into its word embedding.\n",
    "        Args:\n",
    "            token: str, the token to embed\n",
    "\n",
    "        Returns: np.ndarray with the embedded token or None if the token is not in the vocabulary\n",
    "        \"\"\"\n",
    "        embedding = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return embedding\n",
    "\n",
    "    def embed_sentence(self, tokens, reduction='none'):\n",
    "        \"\"\"\n",
    "        Embed a sentence (list of tokens) into word embeddings. Reduction can be 'none', 'mean', or 'sum'.\n",
    "        If 'reduction' is 'none', returns a 2D array of shape (len(tokens), embedding_dim). If 'mean' or 'sum',\n",
    "        returns a 1D array of shape (embedding_dim,) with the values averaged or summed respectively.\n",
    "        Args:\n",
    "            tokens: list of str\n",
    "            reduction: str, one of 'none', 'mean', 'sum'\n",
    "\n",
    "        Returns: np.ndarray with the embedded sentence\n",
    "        \"\"\"\n",
    "        embedding = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove50_model = WordEmbeddings(tokens, embeddings)\n",
    "del tokens, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the method `embed_sentence()`. Notice the shape of the returned arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding = glove50_model.embed_sentence('how are you doing ?'.split(' '))\n",
    "print(embedding.shape)\n",
    "print(embedding[:, :10])\n",
    "embedding = glove50_model.embed_sentence('how are you doing ?'.split(' '), reduction='mean')\n",
    "print(embedding.shape)\n",
    "print(embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Similarities between words\n",
    "\n",
    "The function below returns the most similar words to the word provided. The returned list does not contain the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar_words(word, model:WordEmbeddings, k):\n",
    "    \"\"\"\n",
    "    Finds the k most similar words to the given word using cosine similarity.\n",
    "    The returned list should contain tuples (word, similarity) sorted by similarity (descending from the most similar).\n",
    "    Args:\n",
    "        word: str, the word to find similar words for\n",
    "        model: WordEmbeddings, the word embeddings model\n",
    "        k: int, the number of similar words to return\n",
    "\n",
    "    Returns: list of tuples (word, similarity)\n",
    "    \"\"\"\n",
    "    embedding = model.embed_token(word)\n",
    "    if embedding is None:\n",
    "        return []\n",
    "\n",
    "    word_id = model.token_to_id[word]\n",
    "    all_embeddings = model.embeddings\n",
    "    similarity = cosine_similarity_1_to_n(embedding, all_embeddings)\n",
    "    top_indices = top_k_indices(similarity, k=k + 1).tolist()\n",
    "    most_similar = []\n",
    "    for id in top_indices:\n",
    "        if id == word_id:\n",
    "            continue\n",
    "        most_similar.append((model.id_to_token[id], similarity[id].item()))\n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(most_similar_words('what', glove50_model, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next function contains the code to plot a similarity matrix between multiple words (e.g. if we want to compare 10 words and their pair-wise similarities). It requires a matrix with similarities (as input) and labels (aka the words) to display in the final figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(matrix, labels):\n",
    "    \"\"\"\n",
    "    Displays a plot of the `matrix` of size (N x N) with the labels specified as a list of size N\n",
    "    Args:\n",
    "        matrix: a square-sized (N x N) numpy array\n",
    "        labels: a list of strings of hte size N\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(labels)), labels=labels)\n",
    "    ax.set_yticks(np.arange(len(labels)), labels=labels)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax.text(j, i, f'{matrix[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e13'></a>\n",
    "#### Exercise 13: Plotting similarities between words\n",
    "\n",
    "In the following, we will explore some properties of word embeddings through some examples. We will use 6 example words for this purpose but experiment with other set of words as well. Fill in the next cell to create a similarity matrix between a list of words.\n",
    "\n",
    "Experiment with different words and their similarities plotted. Try at least 2 more (different) sets of words of at least 6 words each. Use the `plot_similarity_matrix` function to visualize the results.\n",
    "Comment on the results. Do they make sense? Why some words are closer to each other than others? What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_of_words = ['love', 'hate', 'life', 'equal', 'alive', 'dead']\n",
    "\n",
    "similarity_matrix = np.zeros((len(list_of_words), len(list_of_words)), dtype=float)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "plot_similarity_matrix(similarity_matrix, list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Back to Sentence Embeddings\n",
    "\n",
    "Let us go back to embedding the whole sentences by averaging the embeddings in the sentence. Below you can find a code snippet that uses our `embed_text()` function and glove model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = 'fox and deer'\n",
    "print(query)\n",
    "\n",
    "query_embedding = embed_text(query, clean, tokenize, lambda x: glove50_model.embed_sentence(x, reduction='mean'))\n",
    "print(query_embedding.shape)\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e14'></a>\n",
    "#### Exercise 14: Analyze sentence embeddings\n",
    "- Calculate similarity between the word embeddings representations of the selected queries and the dataset sentences.\n",
    "- Analyze the search results. Does the search work as expected? Discuss the results.\n",
    "- Compare the results with the ones you got with the bag-of-words and TF-IDF representation. Discuss the differences and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Evaluating Retrieval\n",
    "\n",
    "In this last section we will try to evaluate how good our sentence retrieval system is. To keep the computational resources manageable, we will use the test set for that as its size is more manageable.\n",
    "\n",
    "Recall from the lecture in IR that there are several metrics to evaluate retrieval performance by taking into account the relevance of the retrieved results to the query. We will use Recall@K here (for more metrics and more details refer to the lecture slides and the textbooks).\n",
    "\n",
    "Recall@K is a metric used to measure the effectiveness of a search system in retrieving relevant documents within the top $K$ retrieved documents. It calculates the proportion of relevant documents retrieved within the top-$K$ results, compared to the total number of relevant documents in the collection.\n",
    "\n",
    "$\n",
    "\\text{Recall@K} = \\frac{\\text{Number of relevant documents retrieved in the top }-K}{\\text{Total number of relevant documents}}\n",
    "$\n",
    "\n",
    "In our case, we have a sentence, and it's compressed version. To test our system, we will treat compressed sentences as the queries. Each query will have only a single relevant sentence - the corresponding uncompressed sentence.\n",
    "\n",
    "Therefore, for the calculation of Recall@K we will take into account whether the correct retrieved result is contained within the first $K$ retrieved results. For example, if for a query (i.e. a compressed sentence) we retrieve 10 results and within these we see the relevant one (i.e. the full sentence), then Recall@10 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e15'></a>\n",
    "### Exercise 15: Cosine similarity between two sets of vectors\n",
    "\n",
    "In this exercise you will revisit your implementation of the cosine similarity. Generalize it so that it can accept two arrays containing two sets of vectors (first one containing $M$ vectors and the second one $N$ vectors). Compute the cosine similarity between each pair of vectors coming from the two sets. The result should be an array of size $M x N$.\n",
    "\n",
    "Once again, try to write an efficient code. This means no loops. Remember the relation between matrix multiplication and dot product. (Depending on your implementation of the previous function calculating cosine similarity, this one can be almost the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_m_to_n(vectors, other_vectors):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a multiple vectors and other vectors.\n",
    "    Args:\n",
    "        vectors: a numpy array representing M number of vectors of D dimensions (of the size MxD)\n",
    "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
    "\n",
    "    Returns: a numpy array of cosine similarity between all the vectors and all the other vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function will use your implementation to calculate Recall@K based on the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_recall(queries, sentences, labels, k, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Calculates recall@k given the embeddings of the queries and sentences.\n",
    "    Assumes that only a single sentence with the same index as query is relevant.\n",
    "    Batching is implemented to avoid high memory usage.\n",
    "    Args:\n",
    "        queries: a numpy array with the embeddings of N queries\n",
    "        sentences: a numpy array with the embeddings of N sentences available for retrieval\n",
    "        k: number of top results to search for the relevant sentence\n",
    "        batch_size: number of queries to process at a time\n",
    "\n",
    "    Returns: calculated recall@k\n",
    "\n",
    "    \"\"\"\n",
    "    n_queries = queries.shape[0]\n",
    "    correct = np.zeros(n_queries, dtype=bool)\n",
    "\n",
    "    with tqdm.tqdm(total=n_queries) as pbar:\n",
    "        for batch_start in range(0, n_queries, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, n_queries)\n",
    "            queries_batch = queries[batch_start:batch_end]\n",
    "            batch_similarity = cosine_similarity_m_to_n(queries_batch, sentences)\n",
    "\n",
    "            for i, similarity_row in enumerate(batch_similarity):\n",
    "                query_index = batch_start + i\n",
    "                top_k = top_k_indices(similarity_row, k=k, sorted=False)\n",
    "                label = labels[query_index]\n",
    "                if label in top_k:\n",
    "                    correct[query_index] = True\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    recall = np.sum(correct) / n_queries\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we embed both the queries and answers from the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query_embeddings = []\n",
    "expected_answers = []\n",
    "for example in tqdm.tqdm(dataset['validation']):\n",
    "    query_tokens = example['query_tokens']\n",
    "    query_embeddings.append(glove50_model.embed_sentence(query_tokens, reduction='mean'))\n",
    "    expected_answers.append(example['answer_id'])\n",
    "query_embeddings = np.stack(query_embeddings, axis=0)\n",
    "expected_answers = np.array(expected_answers)\n",
    "\n",
    "answers_embeddings = []\n",
    "for example in tqdm.tqdm(answers_dataset['validation']):\n",
    "    answer_tokens = example['answer_tokens']\n",
    "    answers_embeddings.append(glove50_model.embed_sentence(answer_tokens, reduction='mean'))\n",
    "answers_embeddings = np.stack(answers_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can use the recall function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall_at_1 = calculate_recall(query_embeddings, answers_embeddings, expected_answers, k=1, batch_size=1000)\n",
    "print(f'\\n{recall_at_1 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e16'></a>\n",
    "### Exercise 16: Evaluating retrieval methods\n",
    "\n",
    "Calculate recall for different values of $K$ for all methods:\n",
    "- BOW,\n",
    "- TF-IDF,\n",
    "- Pre-trained embeddings.\n",
    "- Another pre-trained embeddings (for example with larger embedding vectors)\n",
    "\n",
    "Make sure to test on the `test` split. Discuss the results. Comment on how recall changes based on the value of $K$. Are the results expected or surprising?\n",
    "\n",
    "The deliverable for this whole lab is a scientific poster and this last question should be the main thing you will include in the poster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
