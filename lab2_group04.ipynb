{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP 2026\n",
    "# Lab 2: Word Vectors and Information Retrieval\n",
    "## *alt*-title: üöÄ Project CleanSearch AI, a DOGE initiative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## üèõÔ∏èüêï PRESS RELEASE ‚Äî For Immediate (and Maximum Efficiency) Distribution  \n",
    "\n",
    "### The Department of Outdated Government Encyclopedias (DOGE) Launches Revolutionary NLP Project to Rescue Public Knowledge  \n",
    "\n",
    "**Washington, D.C.** ‚Äî In a bold step toward modernizing the nation‚Äôs most chaotic digital archives, the   **Department of Outdated Government Encyclopedias (DOGE)** today announced the launch of its new initiative:  üöÄ **Project CleanSearch AI**.\n",
    "\n",
    "For decades, citizens have struggled to find simple answers hidden inside massive, noisy, and poorly structured government knowledge repositories.\n",
    "\n",
    "Questions such as:\n",
    "\n",
    "- ‚ÄúWho won the Nobel Prize in 1930?‚Äù  \n",
    "- ‚ÄúWhen did Angola become independent?‚Äù  \n",
    "\n",
    "have resulted in thousands of irrelevant web pages, confusing biographies and excessive scrolling üìâ\n",
    "\n",
    "> *‚ÄúFrankly, our archives are a mess,‚Äù* said a DOGE spokesperson.  \n",
    "> *‚ÄúThey‚Äôre long, noisy and about as searchable as a pile of printed Wikipedia pages thrown into a hurricane.‚Äù*\n",
    "\n",
    "### üß† The Solution  \n",
    "\n",
    "DOGE has assembled an elite team of AI specialists, hired from UM DACS 2nd year bachelor program with the following goals:\n",
    "\n",
    "‚úÖ Clean decades of messy digital text  \n",
    "‚úÖ Extract meaningful knowledge  \n",
    "‚úÖ Replace outdated keyword search with modern **retrieval systems**  \n",
    "‚úÖ Deliver instant, accurate answers to citizens  \n",
    "\n",
    "Using real-world noisy data similar to the government‚Äôs archives, the team will experiment with multiple retrieval models to determine the most efficient approach, methods which have been taught in the fabulous classes of some person quoted as J.S. \n",
    "\n",
    "Whispers across the digital corridors suggest that DOGE may soon supercede the legendary Project 2-2, though DACS management insist these rumours are ‚Äúunder control.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deliverable:\n",
    "\n",
    "- You are asked to deliver **two files only**:\n",
    "  - your executed notebook file (`.ipynb`), and\n",
    "  - your poster (`.pdf`).  \n",
    "  No other files will be taken into consideration.\n",
    "  \n",
    "‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è Each part of the poster will contribute to your grade proportionally to what we present below. If we can't find the relevant part in your notebook (e.g. the figure or the code to support your findings) we will reduce (or even zero-out) your grade for that part.\n",
    "\n",
    "### Instructions for the poster: \n",
    "\n",
    "The final deliverable for this lab is a **scientific poster** presenting your work on building and evaluating a sentence retrieval system using the TriviaQA dataset.\n",
    "- üìè **Size:** A0 or A1  \n",
    "- üß≠ **Orientation:** Portrait or landscape (your choice)  \n",
    "- üìë **Layout:** Clear section structure (e.g., columns or blocks)  \n",
    "\n",
    "#### Your poster should include the following sections:\n",
    "---\n",
    "#### 1Ô∏è‚É£ Problem & Motivation üéØ\n",
    "- Describe the retrieval task (query ‚Üí correct answer document) and the challenges\n",
    "- Briefly introduce the dataset and its challenges  \n",
    "#### 2Ô∏è‚É£ Data Preparation üßπ\n",
    "Explain:\n",
    "- Train / validation / test splitting  \n",
    "- Your cleaning pipeline (at least 6 preprocessing steps)  \n",
    "Include at least one **before vs after cleaning** example.\n",
    "#### 3Ô∏è‚É£ Retrieval Models ü§ñ\n",
    "Present and explain the modes you used:\n",
    "- Bag-of-Words + cosine similarity  \n",
    "- TF-IDF + cosine similarity  \n",
    "- Sentence embeddings (averaged word embeddings)  \n",
    "- [any other model?] \n",
    "Discuss strengths and limitations of each.\n",
    "#### 4Ô∏è‚É£ Qualitative Analysis üîç\n",
    "Provide:\n",
    "- At least **3 successful retrieval examples**  \n",
    "- At least **3 failure cases**  \n",
    "Explain why each worked or failed.\n",
    "#### 5Ô∏è‚É£ Quantitative Evaluation üìä (Main focus)\n",
    "Report **Recall@K** (and possibly other metrics) on the **test set** for all methods:\n",
    "- BOW  \n",
    "- TF-IDF  \n",
    "- Pre-trained embeddings  \n",
    "- [Additional models]  \n",
    "Include relevant table(s) and/or plot(s) and briefly discuss trends.\n",
    "#### 6Ô∏è‚É£ Discussion & Recommendations üí°\n",
    "Conclude with:\n",
    "- Which method you would recommend and why  \n",
    "- Key tradeoffs  \n",
    "- Possible improvements  \n",
    "### üé® Optional Creative Element (Bonus)\n",
    "\n",
    "You may (optionally) present your poster within the fictional storyline of üèõÔ∏è **DOGE ‚Äî Department of Outdated Government Encyclopedias**, where your retrieval system modernizes chaotic national archives and replaces legacy keyword search. Creativity is welcome, but scientific clarity is the priority. We will vote for the \"most creative poster\".\n",
    "\n",
    "---\n",
    "\n",
    "### üìè Evaluation Focus\n",
    "\n",
    "Posters will be assessed on:\n",
    "- Correctness of the pipeline incl. the code (25%)\n",
    "- Clarity of explanations and interpretations of results (25%)\n",
    "- Quality of analysis (20%)\n",
    "- Proper use of evaluation metrics (e.g. Recall@K) (10%)\n",
    "- Visual organization (10%)\n",
    "- Discussion and recommendations (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "As in the last lab, we will be using huggingface datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). We will work with TriviaQA dataset ([https://huggingface.co/datasets/sentence-transformers/trivia-qa](https://huggingface.co/datasets/sentence-transformers/trivia-qa)), which contains pairs of queries and articles that contain the answer.\n",
    "\n",
    "In this section we will prepare the dataset, aka clean the sentences and tokenize. We will additionally extract the answers, as some articles correspond to multiple queries. We will create a separate dataset from the unique answers. We will do that for each split separately, so that we can test our retrieval fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start with importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from datasets import DatasetDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading\n",
    "Now, we can begin loading the dataset and inspecting the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('sentence-transformers/trivia-qa')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Splitting\n",
    "\n",
    "You might have noticed that the dataset is not split into subsets (it contains only the `train` subset). To maintain the good practice of working with ML, we should have three datasets: `train`, `validation`, and `test`. The code below splits our dataset into those three subsets. We set the size of both the `validation` and `test` sets as 10,000 and keep the rest in the `train` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=10_000)\n",
    "valid_dataset = dataset['test']\n",
    "dataset = dataset['train'].train_test_split(test_size=10_000)\n",
    "dataset['validation'] = valid_dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning\n",
    "\n",
    "Let's write the function to clean the text. It can be similar to the one from the previous lab (Lab1) but make sure that it makes sense for this dataset and task.\n",
    "\n",
    "More specifically, think about lower-casing, punctuation, stop-words and lemmatization/stemming and the impact it might have on the dataset. Also reflect on the fact that with word embeddings we want to uncover semantic relationships between words, whereas with bag-of-words we were trying to capture different morphological variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise 1: Clean function\n",
    "Fill in the following function to clean the dataset. Implement at least 6 different steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Cleans the text\n",
    "    Args:\n",
    "        text: a string that will be cleaned\n",
    "\n",
    "    Returns: the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        return text\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "sentence = 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?'\n",
    "print('Testing the clean function:')\n",
    "print('Original:', sentence)\n",
    "print('Cleaned:', clean(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function will apply the function you just wrote to the whole dataset. More specifically, it takes the `query` and `answer` fields, applies the `clean` function and saves the processed sentences back to the `query` and `answer` fields. This will override the original fields. If you want to have access to them, you can make a copy in separate fields before cleaning. As in the last lab, we will use the `map()` method of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_example(example):\n",
    "    \"\"\"\n",
    "    Applies the clean() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example with cleaned 'query' and 'answer' columns\n",
    "\n",
    "    \"\"\"\n",
    "    # example['original_query'] = example['query']\n",
    "    # example['original_answer'] = example['answer']\n",
    "\n",
    "    example['query'] = clean(example['query'])\n",
    "    example['answer'] = clean(example['answer'])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(clean_example, desc=\"Cleaning queries and answers\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine some examples from the dataset and make sure that we got the results we wanted. At this step, it might be necessary to revisit some pre-processing steps if you are not happy with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('example', i)\n",
    "    print('query', dataset['train'][i]['query'])\n",
    "    print('answer', dataset['train'][i]['answer'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extracting answers\n",
    "\n",
    "Because the answers in our dataset are not unique, we will extract them and create a separate dataset containing only the unique answers. We will do this for each split separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_answers(subset):\n",
    "    \"\"\"\n",
    "    Extracts unique answers from the subset of the dataset and builds a dictionary with answers as keys and ids as values.\n",
    "    Args:\n",
    "        subset: a subset of the dataset\n",
    "\n",
    "    Returns: a dictionary mapping answers to their ids\n",
    "    \"\"\"\n",
    "    answer_to_id = {}\n",
    "    answers = list(set(subset['answer']))\n",
    "    for i, answer in enumerate(answers):\n",
    "        answer_to_id[answer] = i\n",
    "    return answer_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply this function separately to each subset and create the answers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_answer_to_id = get_answers(dataset['train'])\n",
    "valid_answer_to_id = get_answers(dataset['validation'])\n",
    "test_answer_to_id = get_answers(dataset['test'])\n",
    "\n",
    "answers_dataset = DatasetDict({\n",
    "    'train': datasets.Dataset.from_dict({'id': range(len(train_answer_to_id)), 'answer': train_answer_to_id.keys()}),\n",
    "    'validation': datasets.Dataset.from_dict(\n",
    "        {'id': range(len(valid_answer_to_id)), 'answer': valid_answer_to_id.keys()}),\n",
    "    'test': datasets.Dataset.from_dict({'id': range(len(test_answer_to_id)), 'answer': test_answer_to_id.keys()})\n",
    "})\n",
    "print(answers_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The last thing we will have to do is to connect the answers in the original dataset to the ids of answers (in the answers dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise 2: Setting answer ids\n",
    "Fill in the following function to find and set the `answer_id` field with the id of the answer. The function accepts one of the `answer_to_id` dictionaries that you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_answer_id(example, answer_to_id):\n",
    "    \"\"\"\n",
    "    Sets the answer_id field in the example based on the answer_to_id dictionary\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        answer_to_id: a dictionary mapping answers to their ids\n",
    "\n",
    "    Returns: the updated example with the 'answer_id' field\n",
    "    \"\"\"\n",
    "    answer = example['answer']\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we apply the function to each split separately making sure to pass the correct `answer_to_id` dictionary. We also remove the `answer` columns from the original dataset, as now we can reference the correct answer through the `answer_id` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].map(set_answer_id,\n",
    "                                        fn_kwargs={'answer_to_id': train_answer_to_id},\n",
    "                                        desc=\"Setting ids for answers (train)\")\n",
    "dataset['validation'] = dataset['validation'].map(set_answer_id,\n",
    "                                                  fn_kwargs={'answer_to_id': valid_answer_to_id},\n",
    "                                                  desc=\"Setting ids for answers (validation)\")\n",
    "dataset['test'] = dataset['test'].map(set_answer_id,\n",
    "                                      fn_kwargs={'answer_to_id': test_answer_to_id},\n",
    "                                      desc=\"Setting ids for answers (test\")\n",
    "\n",
    "dataset = dataset.remove_columns('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizing\n",
    "\n",
    "<a name='e3'></a>\n",
    "#### Exercise 3: Tokenizing\n",
    "As always, we will need to tokenize the dataset in order to create bat-of-words and TF-IDF representations in the next sections. You can use the function from the previous lab or use a library such as [Natural Language Toolkit (NLTK) library]([https://www.nltk.org/]) (https://www.nltk.org/). Complete the following function to split the text into tokens.\n",
    "\n",
    "Contrary to the previous lab, we will not include the special tokens (unknown, beginning, and end of the sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text that is assumed to be cleaned first with the clean() function. The tokenized sequence should start with the `bos_token` token and end with the 'eos_token'.\n",
    "    Args:\n",
    "        text: a cleaned text\n",
    "\n",
    "    Returns: tokenized text as a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = None  # list of tokens, your code should fill this variable\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply your function to both the `query` field in the original dataset and `answer` field in the answers dataset. We save the tokenized queries in `query_tokens` field and answers in `answer_tokens` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_example(example, src_column, tgt_column):\n",
    "    \"\"\"\n",
    "    Applies the tokenize() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'query_tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    query = example[src_column]\n",
    "    example[tgt_column] = tokenize(query)\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_example,\n",
    "                      fn_kwargs={'src_column': 'query', 'tgt_column': 'query_tokens'},\n",
    "                      desc=\"Tokenizing queries\")\n",
    "print('dataset')\n",
    "print(dataset)\n",
    "\n",
    "answers_dataset = answers_dataset.map(tokenize_example,\n",
    "                                      fn_kwargs={'src_column': 'answer', 'tgt_column': 'answer_tokens'},\n",
    "                                      desc=\"Tokenizing answers\")\n",
    "print('answers_dataset')\n",
    "print(answers_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine some examples of tokenized queries and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('example:', i)\n",
    "    print('query:', dataset['train'][i]['query'])\n",
    "    print('query_tokens:', dataset['train'][i]['query_tokens'])\n",
    "    answer_id = dataset['train'][i]['answer_id']\n",
    "    print('answer_id:', answer_id)\n",
    "    print('answer:', answers_dataset['train'][answer_id]['answer'])\n",
    "    print('answer_tokens:', answers_dataset['train'][answer_id]['answer_tokens'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice the difference in the types of the different structures we use. Run the following cell to check the types. Do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#type of original dataset\n",
    "print(type(dataset))\n",
    "print(\"--\")\n",
    "#type of the split of the dataset\n",
    "print(type(dataset['test']))\n",
    "print(\"--\")\n",
    "#type of original query\n",
    "print(dataset['train'][0]['query'])\n",
    "print(type(dataset['train'][0]['query']))\n",
    "print(\"--\")\n",
    "#type of tokenized query\n",
    "print(dataset['train'][0]['query_tokens'])\n",
    "print(type(dataset['train'][0]['query_tokens']))\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "In this section you will built a bag-of-words representation of the dataset. We will use numpy arrays to store the results. The bag-of-words representation is a simple and effective way to represent text data. It involves creating a vocabulary of unique words from the dataset and representing each sentence as a vector of word counts. We first need the vocabulary, which we will build from both the full sentences and the compressed sentences. Similar to the first lab, the vocabulary will be a list of unique words from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extracting Vocabulary\n",
    "\n",
    "<a name='e4'></a>\n",
    "#### Exercise 4: Extracting vocabulary counts\n",
    "\n",
    "In the following cell, you will implement a function that takes two datasets (`dataset`, and `answers_dataset`) and returns a dictionary with the counts of each word in the vocabulary. The dictionary should be of the form {word: count}. As in previous lab, you will use the `Counter` class from the `collections` module to do this. Iterate over the two datasets and count the tokens in `query_tokens` and `answer_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary_counts(dataset, answers_dataset):\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from the tokenized sentences\n",
    "    Args:\n",
    "        dataset: a Dataset from which 'query_tokens' are used to build vocabulary\n",
    "        answers_dataset: a Dataset from which 'answer_tokens' are used to build vocabulary\n",
    "\n",
    "    Returns: a Counter object with the counts of each word in the vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = Counter()\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we use the function you implemented. Notice that we build our vocabulary based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_counter = extract_vocabulary_counts(dataset['train'], answers_dataset['train'])\n",
    "print(len(vocab_counter))\n",
    "print(vocab_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will truncate the vocabulary. We also create the handy `token_to_id` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 20_000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "token_to_id = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "\n",
    "<a name='e5'></a>\n",
    "#### Exercise 5: Bag of Words\n",
    "Here we will create the bag-of-words representation of the sentences. The function will take a single sentence (list of tokens) and return an array of size `vocab_size` with the counts of each word in the vocabulary. The\n",
    "`vocab_size` is calculated as the length of the passed `token_to_id` dictionary. The resulting array should have zeros everywhere but the indices corresponding to the words in the vocabulary where it should have the counts of the words in the sentence. For example, if the sentence is `['fox', 'and', 'deer']` and the vocabulary is `{'fox': 0, 'and': 1, 'deer': 2}`, the resulting array should be `[1, 1, 1]`. If the sentence is `['fox', 'and', 'fox', 'deer']`, the resulting array should be `[2, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(sentence_tokens, token_to_id):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of the sentence\n",
    "    Args:\n",
    "        sentence_tokens: a list of tokens\n",
    "        token_to_id: a dictionary mapping each word to an index in the vocabulary\n",
    "\n",
    "    Returns:: a numpy array of size vocab_size with the counts of each word in the vocabulary\n",
    "    \"\"\"\n",
    "    vocab_size = len(token_to_id)\n",
    "    bow = np.zeros(vocab_size, dtype=int)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the function. The output should be a numpy array of size `vocab_size` with the counts of each word in the vocabulary. Notice that most of the elements of the BOW representation are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Tokenized sentence:')\n",
    "print(dataset['test'][0]['query_tokens'])\n",
    "query_bow = bag_of_words(dataset['test'][0]['query_tokens'], token_to_id)\n",
    "query_non_zero_bow = np.nonzero(query_bow)[0]\n",
    "\n",
    "print('Bag of words:')\n",
    "print(query_bow)\n",
    "print('Type of bag of words:')\n",
    "print(type(query_bow))\n",
    "print('Shape of bag of words:')\n",
    "print(query_bow.shape)\n",
    "print('Non-zero elements in bag of words:')\n",
    "print(query_non_zero_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine further the non-zero elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Non-zero elements in bag of words:')\n",
    "print(query_non_zero_bow)\n",
    "for i in query_non_zero_bow:\n",
    "    print(vocab[i], ':', query_bow[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Function for Embedding Text\n",
    "\n",
    "The following function will apply all the steps we implemented to a single sentence. It returns a bag of words representation that we will use to calculate the similarity between different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def embed_text(text, clean_fn, tokenize_fn, embed_fn):\n",
    "    \"\"\"\n",
    "    Embeds the text using the provided functions. The pipeline applies cleaning (clean_fn), tokenization (tokenize_fn), and embedding (embed_fn).\n",
    "    Args:\n",
    "        text: the text to be embedded\n",
    "        clean_fn: function/Callable clean_fn(text:str):str\n",
    "        tokenize_fn: function/Callable tokenize_fn(text:str): List[str]\n",
    "        embed_fn: function/Callable embed_fn(tokens:List[str]): np.ndarray\n",
    "\n",
    "    Returns: the embedding of the text as a numpy array\n",
    "    \"\"\"\n",
    "    cleaned = clean_fn(text)\n",
    "    tokens = tokenize_fn(cleaned)\n",
    "    embedding = embed_fn(tokens)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "embedding = embed_text(\"This is an example of a sentence\", clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "print(embedding.shape)\n",
    "print(np.nonzero(embedding)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "<a name='e6'></a>\n",
    "#### Exercise 6: Cosine Similarity between two vectors\n",
    "\n",
    "Complete the following function that given any two vectors will compute the cosine similarity. If you don't remember the formula for the cosine similarity, revisit the course material. Notice that the function receives numpy arrays and recall that you can express cosine similarity as a dot product. Use numpy functions to write an efficient implementation. Two more exercises builds upon this one, so make sure to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    Args:\n",
    "        vector1: numpy array of the first vector\n",
    "        vector2: numpy array of the second vector\n",
    "\n",
    "    Returns: cosine similarity\n",
    "\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity(np.array([0, 1, 2]), np.array([0, 2, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how similar are the BOW representations of some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'Some interesting document containing sentences.',\n",
    "    'The quick brown fox jumps over the lazy cat and some other stuff.',\n",
    "    'Fox and deer are not friends.',\n",
    "    'Fox and deer are not friends. But this document is a lot longer than the previous one. We can add sentence by sentence and see how the embeddings change.',\n",
    "]\n",
    "embedded_sentences = [\n",
    "    embed_text(sentence, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "query = 'fox and deer'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "cosine_similarities = [\n",
    "    cosine_similarity(embedded_query, embedded_sentence)\n",
    "    for embedded_sentence in embedded_sentences\n",
    "]\n",
    "print(f'Query: {query}')\n",
    "for sent, cos_sim in zip(sentences, cosine_similarities):\n",
    "    print(f'Cosine Similarity: {cos_sim:.4f} - Sentence: {sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "In this section, we will use the BOW representations to finally search for the answers to our questions. We start by calculating the BOWs of queries and answers of the whole `validation` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "valid_queries_bows = []\n",
    "for example in tqdm.tqdm(dataset['validation']):\n",
    "    valid_queries_bows.append(bag_of_words(example['query_tokens'], token_to_id))\n",
    "\n",
    "valid_answers_bows = []\n",
    "for example in tqdm.tqdm(answers_dataset['validation']):\n",
    "    valid_answers_bows.append(bag_of_words(example['answer_tokens'], token_to_id))\n",
    "\n",
    "valid_queries_bows = np.array(valid_queries_bows)\n",
    "valid_answers_bows = np.array(valid_answers_bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e7'></a>\n",
    "#### Exercise 7: Cosine Similarity between a vector and an array of vectors\n",
    "\n",
    "The next step in our retrieval system, would be to calculate the proximity of a query to our retrieval corpus (in our case that is all the sentences).\n",
    "\n",
    "Complete the following function to calculate the cosine similarity between a vector (first parameter `vector`, that will usually be the query vector) and all other vectors (second parameter `other_vectors`, that will be the sentence embeddings in our case). Note that the `other_vectors` parameter is a single numpy array of size `N x D`, where $N$ is the number of vectors and $D$ is the dimension of each vector.\n",
    "\n",
    "For maximum efficiency (we will need it) do not use loops. Try to write the implementation with numpy functions. Hint: matrix multiplication can be seen as calculating the dot product between rows and columns of the multiplied matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_1_to_n(vector, other_vectors):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a single vector and other vectors.\n",
    "    Args:\n",
    "        vector: a numpy array representing a vector of D dimensions\n",
    "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
    "\n",
    "    Returns: a 1D numpy array of size N containing the cosine similarity between the vector and all the other vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now can try out our retrieval system by calculating the cosine similarities between the query and all answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = 'Which vegetable is Blackadder‚Äôs servant obsessed with in the UK television series ‚ÄòBlackadder II‚Äô?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "print(query_similarity.shape)\n",
    "print(query_similarity[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "most_similar = int(np.argmax(query_similarity))\n",
    "print(most_similar)\n",
    "print(query_similarity[most_similar])\n",
    "print(valid_answers_bows[most_similar])\n",
    "print(answers_dataset['validation'][most_similar]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function returns the indices of the top-k elements in the array. If the `sorted` parameter is `True` (it is by default) the returned array will be sorted in the descending order (of the corresponding values in array). For example, if the `array` is `[3, 2, 4, 1]` and `k=2` the returned numpy array will be `[2, 0]` if `sorted` is True (the top values are `3` and `4` with indices `0` and `2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_k_indices(array, k, sorted=True):\n",
    "    \"\"\"\n",
    "    Returns top-k indices from the 1D array. If `sorted` is `True` the returned indices are sorted in the descending order\n",
    "    Args:\n",
    "        array: a 1D numpy array\n",
    "        k: a number of top indices to return\n",
    "        sorted: if True, the returned indices are sorted in descending order\n",
    "\n",
    "    Returns: a 1D numpy array containing top-k indices\n",
    "\n",
    "    \"\"\"\n",
    "    top_k = np.argpartition(array, -k)[-k:]\n",
    "    if sorted:\n",
    "        selected = array[top_k]\n",
    "        sorted_selected = (-selected).argsort()\n",
    "        top_k = top_k[sorted_selected]\n",
    "    return top_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "for idx in top_indices:\n",
    "    print(answers_dataset['validation'][idx]['answer'])\n",
    "    print(f'similarity: {query_similarity[idx]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e8'></a>\n",
    "#### Exercise 8: Analyzing and improving BOW search results\n",
    "\n",
    "Experiment with different queries (taking into account the nature of the dataset and your insights from the analysis so far).\n",
    "Answer the following questions:\n",
    "- Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 3 from each category). Provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling, ...)\n",
    "- If you see problems with search, how could you improve your implementation? Change the functions above, if you think there is room for improvement. Describe your changes and how they made the search better or (in case you made no changes) explain what made the search robust enough to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "In this section we will implement the TF-IDF algorithm. While BOW is a simple way to represent the documents, it has some limitations. For example, it does not take into account the importance of each word in the document. TF-IDF representation takes into account the frequency of each word in the document and the frequency of the word in the whole dataset. It is a widely used technique in information retrieval and text mining. Refer to the lecture slides for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "<a name='e9'></a>\n",
    "#### Exercise 9: Inverse Document Frequency (IDF)\n",
    "In this exercise, you will implement the TF-IDF algorithm. First, calculate Inverse Document Frequency (IDF) for each word in the vocabulary. Intuitively, it is a measure of how informative a word is based on the whole dataset. Consult the lecture slides for the details. The IDF is calculated as follows:\n",
    "$$\n",
    "IDF(t) = log_{10}(N/df(t))$$\n",
    "where $N$ is the total number of documents (sentences) in the dataset and $df(t)$ is the number of documents containing the word $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_idf(bows):\n",
    "    \"\"\"\n",
    "    Calculates the IDF for each word in the vocabulary\n",
    "    Args:\n",
    "        bows: numpty array of size (N x D) where N is the number of documents and D is the vocabulary size\n",
    "\n",
    "    Returns: a numpy array of size D with IDF values for each token\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To avoid the data leakage, the IDF should be calculated the train subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_answers_bows = []\n",
    "for example in tqdm.tqdm(answers_dataset['train']):\n",
    "    train_answers_bows.append(bag_of_words(example['answer_tokens'], token_to_id))\n",
    "\n",
    "train_answers_bows = np.array(train_answers_bows)\n",
    "\n",
    "idf = calculate_idf(train_answers_bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full TF-IDF\n",
    "\n",
    "<a name='e10'></a>\n",
    "#### Exercise 10: TF-IDF\n",
    "- Calculate TF-IDF on the `test` subset of the dataset.\n",
    "- Analyze the search results based on your implemented TF-IDF. Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 3 from each category). Provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling with average embeddings, ...)\n",
    "- Compare the results with the ones you got with the bag-of-words representation. Discuss the differences and similarities. Do you think TF-IDF is a better representation for this task? Why or why not? Provide examples to support your arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# You can implement the following functions, but you can also use your own design.\n",
    "\n",
    "def calculate_tf_idf_n(bows, idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for each word in the vocabulary\n",
    "    Args:\n",
    "        bows: numpty array of size (N x D) where N is the number of documents and D is the vocabulary size\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size (N x D) with TF-IDF values for each document and each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_tf_idf(bow, idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for a single document\n",
    "    Args:\n",
    "        bow: a numpy array of size D with the bag-of-words representation of the document\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size D with TF-IDF values for each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def embed_tf_idf(sentence, token_to_id, idf):\n",
    "    \"\"\"\n",
    "    Embeds the sentence using TF-IDF\n",
    "    Args:\n",
    "        sentence: a list of tokens\n",
    "        token_to_id: a dictionary mapping each word to an index in the vocabulary\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size D with TF-IDF values for each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# query = dataset['validation'][0]['query']\n",
    "# print('query:', query)\n",
    "# query_tfidf = embed_text(query, clean, tokenize, lambda x: embed_tf_idf(x, token_to_id, idf))\n",
    "#\n",
    "# answers_tfidf = calculate_tf_idf_n(valid_answers_bows, idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "\n",
    "Word embeddings are a powerful model for representing words and their meaning (in terms of distributional similarity). As we discussed in class, we can use them in a wide variety of tasks with more complex architectures. Word vectors offer a dense vector for each word.\n",
    "\n",
    "In this section you will load the pre-trained word embeddings model - Glove. You can read more about it [here](https://aclanthology.org/D14-1162/) ([https://aclanthology.org/D14-1162/](https://aclanthology.org/D14-1162/)). The embeddings are trained on a large corpus of text and are available in different dimensions. We will start with the dimension of 100, but later you will be asked to experiment with other dimensions.\n",
    "\n",
    "You can download the embeddings manually from one of the following links:\n",
    "- https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained/data?select=glove.6B.50d.txt\n",
    "- https://github.com/nishankmahore/word2vec-flask-api (check the table in the readme)\n",
    "\n",
    "The extracted files should contain several models but for now we will be using the 50-dimensional one. This means that each token is represented as a 50-dimensional floating point vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_embeddings_path = 'glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will load and parse this file line-by-line. Your job in the next exercise is the parsing part.\n",
    "\n",
    "<a name='e11'></a>\n",
    "#### Exercise 11: Parsing the embeddings\n",
    "Implement the following function to parse a single line of the glove embeddings file. The line contains string values separated by spaces. The first value is the token and the rest are the elements of the embedding vector (the values should be cast to float). You can inspect the file to get a better idea of what is there. Return both the token (as string) and the embedding (as a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_embeddings_row(line):\n",
    "    \"\"\"\n",
    "    Parses a single line from the GloVe embeddings file. The line contains a word followed by its embedding values separated by spaces.\n",
    "    Args:\n",
    "        line: a line from the GloVe embeddings file\n",
    "\n",
    "    Returns: a tuple (word, embedding) where 'word' is a string and 'embedding' is a numpy array of floats\n",
    "    \"\"\"\n",
    "    line_split = line.split()\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return token, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we load the file and iterate over the lines to load the tokens and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "embeddings = []\n",
    "with open(glove_embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        token, embedding = parse_embeddings_row(line)\n",
    "        tokens.append(token)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.stack(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will create a class `WordEmbeddings` that will hold embeddings and expose useful methods to embed a single token (`embed_token()`) and the whole sentence (`embed_sentence()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sentence Embeddings by Averaging Word Embeddings\n",
    "In the course, we will see different architectures that take into account the sequence of words (by combining their vectors). A first naive but simple and sometimes (as we are going to see) quite effective approach would be to represent a sentence with an embedding vector that is the average of the word vectors that form the sentence.\n",
    "\n",
    "So formally, this is what we are aiming for:\n",
    "\n",
    "$\n",
    "\\text{Sentence_Embedding} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Word_Embedding}_i\n",
    "$\n",
    "\n",
    "where:\n",
    "* $N$ is the number of words in a sentence\n",
    "* $\\text{Word_Embedding}_i$ is the word vector for the $i$-th in the sentence.\n",
    "\n",
    "Things to note:\n",
    "* The embedding vector for the sentence will obviously have the same dimension as the word embedding.\n",
    "* This representation ignores the word order (like bag-of-words). During the course we will see how we can overcome this limitation by using sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e12'></a>\n",
    "#### Exercise 12: Word Embeddings Class\n",
    "Implement the two methods in the following class:\n",
    "- `embed_token()` - accepts a token to be embedded. Use `self.token_to_id` to find the row in the `self.embeddings` attribute. If the token is outside the vocabulary, return `None`.\n",
    "- `embed_sentence()` - accepts a list of tokens that form a sentence. Each token (that is inside the vocabulary) should be embedded. The second parameter `reduction` will determine how the embedded tokens are \"reduced\". There are three options: `mean` should average the tokens (resulting in a single numpy array of size `embedding_dim`, which is 50 for our model), `sum` should sum the tokens, and `none` should return the embeddings of all tokens as a single numpy array (the array should be of shape `(N, embedding_dim)`). Think about a situation where none of the tokens in a sentence is in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordEmbeddings:\n",
    "    def __init__(self, vocabulary, embeddings):\n",
    "        \"\"\"\n",
    "        Initializes the WordEmbeddings object.\n",
    "        Args:\n",
    "            vocabulary: list of str\n",
    "            embeddings: np.ndarray of shape (vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        self.token_to_id = {token: i for i, token in enumerate(vocabulary)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(vocabulary)}\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def embed_token(self, token):\n",
    "        \"\"\"\n",
    "        Embed a single token into its word embedding.\n",
    "        Args:\n",
    "            token: str, the token to embed\n",
    "\n",
    "        Returns: np.ndarray with the embedded token or None if the token is not in the vocabulary\n",
    "        \"\"\"\n",
    "        embedding = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return embedding\n",
    "\n",
    "    def embed_sentence(self, tokens, reduction='none'):\n",
    "        \"\"\"\n",
    "        Embed a sentence (list of tokens) into word embeddings. Reduction can be 'none', 'mean', or 'sum'.\n",
    "        If 'reduction' is 'none', returns a 2D array of shape (len(tokens), embedding_dim). If 'mean' or 'sum',\n",
    "        returns a 1D array of shape (embedding_dim,) with the values averaged or summed respectively.\n",
    "        Args:\n",
    "            tokens: list of str\n",
    "            reduction: str, one of 'none', 'mean', 'sum'\n",
    "\n",
    "        Returns: np.ndarray with the embedded sentence\n",
    "        \"\"\"\n",
    "        embedding = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove50_model = WordEmbeddings(tokens, embeddings)\n",
    "del tokens, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the method `embed_sentence()`. Notice the shape of the returned arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding = glove50_model.embed_sentence('how are you doing ?'.split(' '))\n",
    "print(embedding.shape)\n",
    "print(embedding[:, :10])\n",
    "embedding = glove50_model.embed_sentence('how are you doing ?'.split(' '), reduction='mean')\n",
    "print(embedding.shape)\n",
    "print(embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Similarities between words\n",
    "\n",
    "The function below returns the most similar words to the word provided. The returned list does not contain the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar_words(word, model:WordEmbeddings, k):\n",
    "    \"\"\"\n",
    "    Finds the k most similar words to the given word using cosine similarity.\n",
    "    The returned list should contain tuples (word, similarity) sorted by similarity (descending from the most similar).\n",
    "    Args:\n",
    "        word: str, the word to find similar words for\n",
    "        model: WordEmbeddings, the word embeddings model\n",
    "        k: int, the number of similar words to return\n",
    "\n",
    "    Returns: list of tuples (word, similarity)\n",
    "    \"\"\"\n",
    "    embedding = model.embed_token(word)\n",
    "    if embedding is None:\n",
    "        return []\n",
    "\n",
    "    word_id = model.token_to_id[word]\n",
    "    all_embeddings = model.embeddings\n",
    "    similarity = cosine_similarity_1_to_n(embedding, all_embeddings)\n",
    "    top_indices = top_k_indices(similarity, k=k + 1).tolist()\n",
    "    most_similar = []\n",
    "    for id in top_indices:\n",
    "        if id == word_id:\n",
    "            continue\n",
    "        most_similar.append((model.id_to_token[id], similarity[id].item()))\n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(most_similar_words('what', glove50_model, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next function contains the code to plot a similarity matrix between multiple words (e.g. if we want to compare 10 words and their pair-wise similarities). It requires a matrix with similarities (as input) and labels (aka the words) to display in the final figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(matrix, labels):\n",
    "    \"\"\"\n",
    "    Displays a plot of the `matrix` of size (N x N) with the labels specified as a list of size N\n",
    "    Args:\n",
    "        matrix: a square-sized (N x N) numpy array\n",
    "        labels: a list of strings of hte size N\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(labels)), labels=labels)\n",
    "    ax.set_yticks(np.arange(len(labels)), labels=labels)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax.text(j, i, f'{matrix[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e13'></a>\n",
    "#### Exercise 13: Plotting similarities between words\n",
    "\n",
    "In the following, we will explore some properties of word embeddings through some examples. We will use 6 example words for this purpose but experiment with other set of words as well. Fill in the next cell to create a similarity matrix between a list of words.\n",
    "\n",
    "Experiment with different words and their similarities plotted. Try at least 2 more (different) sets of words of at least 6 words each. Use the `plot_similarity_matrix` function to visualize the results.\n",
    "Comment on the results. Do they make sense? Why some words are closer to each other than others? What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_of_words = ['love', 'hate', 'life', 'equal', 'alive', 'dead']\n",
    "\n",
    "similarity_matrix = np.zeros((len(list_of_words), len(list_of_words)), dtype=float)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "plot_similarity_matrix(similarity_matrix, list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Back to Sentence Embeddings\n",
    "\n",
    "Let us go back to embedding the whole sentences by averaging the embeddings in the sentence. Below you can find a code snippet that uses our `embed_text()` function and glove model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = 'fox and deer'\n",
    "print(query)\n",
    "\n",
    "query_embedding = embed_text(query, clean, tokenize, lambda x: glove50_model.embed_sentence(x, reduction='mean'))\n",
    "print(query_embedding.shape)\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e14'></a>\n",
    "#### Exercise 14: Analyze sentence embeddings\n",
    "- Calculate similarity between the word embeddings representations of the selected queries and the dataset sentences.\n",
    "- Analyze the search results. Does the search work as expected? Discuss the results.\n",
    "- Compare the results with the ones you got with the bag-of-words and TF-IDF representation. Discuss the differences and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Evaluating Retrieval\n",
    "\n",
    "In this last section we will try to evaluate how good our sentence retrieval system is. To keep the computational resources manageable, we will use the test set for that as its size is more manageable.\n",
    "\n",
    "Recall from the lecture in IR that there are several metrics to evaluate retrieval performance by taking into account the relevance of the retrieved results to the query. We will use Recall@K here (for more metrics and more details refer to the lecture slides and the textbooks).\n",
    "\n",
    "Recall@K is a metric used to measure the effectiveness of a search system in retrieving relevant documents within the top $K$ retrieved documents. It calculates the proportion of relevant documents retrieved within the top-$K$ results, compared to the total number of relevant documents in the collection.\n",
    "\n",
    "$\n",
    "\\text{Recall@K} = \\frac{\\text{Number of relevant documents retrieved in the top }-K}{\\text{Total number of relevant documents}}\n",
    "$\n",
    "\n",
    "In our case, we have a sentence, and it's compressed version. To test our system, we will treat compressed sentences as the queries. Each query will have only a single relevant sentence - the corresponding uncompressed sentence.\n",
    "\n",
    "Therefore, for the calculation of Recall@K we will take into account whether the correct retrieved result is contained within the first $K$ retrieved results. For example, if for a query (i.e. a compressed sentence) we retrieve 10 results and within these we see the relevant one (i.e. the full sentence), then Recall@10 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e15'></a>\n",
    "### Exercise 15: Cosine similarity between two sets of vectors\n",
    "\n",
    "In this exercise you will revisit your implementation of the cosine similarity. Generalize it so that it can accept two arrays containing two sets of vectors (first one containing $M$ vectors and the second one $N$ vectors). Compute the cosine similarity between each pair of vectors coming from the two sets. The result should be an array of size $M x N$.\n",
    "\n",
    "Once again, try to write an efficient code. This means no loops. Remember the relation between matrix multiplication and dot product. (Depending on your implementation of the previous function calculating cosine similarity, this one can be almost the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_m_to_n(vectors, other_vectors):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a multiple vectors and other vectors.\n",
    "    Args:\n",
    "        vectors: a numpy array representing M number of vectors of D dimensions (of the size MxD)\n",
    "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
    "\n",
    "    Returns: a numpy array of cosine similarity between all the vectors and all the other vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function will use your implementation to calculate Recall@K based on the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_recall(queries, sentences, labels, k, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Calculates recall@k given the embeddings of the queries and sentences.\n",
    "    Assumes that only a single sentence with the same index as query is relevant.\n",
    "    Batching is implemented to avoid high memory usage.\n",
    "    Args:\n",
    "        queries: a numpy array with the embeddings of N queries\n",
    "        sentences: a numpy array with the embeddings of N sentences available for retrieval\n",
    "        k: number of top results to search for the relevant sentence\n",
    "        batch_size: number of queries to process at a time\n",
    "\n",
    "    Returns: calculated recall@k\n",
    "\n",
    "    \"\"\"\n",
    "    n_queries = queries.shape[0]\n",
    "    correct = np.zeros(n_queries, dtype=bool)\n",
    "\n",
    "    with tqdm.tqdm(total=n_queries) as pbar:\n",
    "        for batch_start in range(0, n_queries, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, n_queries)\n",
    "            queries_batch = queries[batch_start:batch_end]\n",
    "            batch_similarity = cosine_similarity_m_to_n(queries_batch, sentences)\n",
    "\n",
    "            for i, similarity_row in enumerate(batch_similarity):\n",
    "                query_index = batch_start + i\n",
    "                top_k = top_k_indices(similarity_row, k=k, sorted=False)\n",
    "                label = labels[query_index]\n",
    "                if label in top_k:\n",
    "                    correct[query_index] = True\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    recall = np.sum(correct) / n_queries\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we embed both the queries and answers from the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query_embeddings = []\n",
    "expected_answers = []\n",
    "for example in tqdm.tqdm(dataset['validation']):\n",
    "    query_tokens = example['query_tokens']\n",
    "    query_embeddings.append(glove50_model.embed_sentence(query_tokens, reduction='mean'))\n",
    "    expected_answers.append(example['answer_id'])\n",
    "query_embeddings = np.stack(query_embeddings, axis=0)\n",
    "expected_answers = np.array(expected_answers)\n",
    "\n",
    "answers_embeddings = []\n",
    "for example in tqdm.tqdm(answers_dataset['validation']):\n",
    "    answer_tokens = example['answer_tokens']\n",
    "    answers_embeddings.append(glove50_model.embed_sentence(answer_tokens, reduction='mean'))\n",
    "answers_embeddings = np.stack(answers_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can use the recall function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall_at_1 = calculate_recall(query_embeddings, answers_embeddings, expected_answers, k=1, batch_size=1000)\n",
    "print(f'\\n{recall_at_1 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e16'></a>\n",
    "### Exercise 16: Evaluating retrieval methods\n",
    "\n",
    "Calculate recall for different values of $K$ for all methods:\n",
    "- BOW,\n",
    "- TF-IDF,\n",
    "- Pre-trained embeddings.\n",
    "- Another pre-trained embeddings (for example with larger embedding vectors)\n",
    "\n",
    "Make sure to test on the `test` split. Discuss the results. Comment on how recall changes based on the value of $K$. Are the results expected or surprising?\n",
    "\n",
    "The deliverable for this whole lab is a scientific poster and this last question should be the main thing you will include in the poster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
